{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check List 3.2 - Combined Detector (ArucoStero, MultiICP)  \n",
    "\n",
    "* This test is for checking combined detection functionality with Aruco and MultiICP. Two cameras and one cup is used as shown below. \n",
    "\n",
    "\n",
    "* **3.2.1 Combined Detector**  \n",
    "  - initialize, set_config, detect_and_register, disconnect   \n",
    "\n",
    "* **Test setup**\n",
    "  - Camera: Kinect (Main), Realsense(Sub)\n",
    "  - Target: Cup\n",
    "<img src=\"./Figs/3.2.CupDetection.jpg\" width=\"80%\">\n",
    "  \n",
    "* **TBD**\n",
    "  - Auto initialization to estimate initial guess for ICP is not perfect\n",
    "  - Robust and reliable initial guess for gloabl registration will be done\n",
    "  - Multiple instance for the same class will be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set running directory to Project source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.join(os.environ[\"RNB_PLANNING_DIR\"], 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.global_config import RNB_PLANNING_DIR\n",
    "from pkg.utils.utils import *    \n",
    "from pkg.utils.rotation_utils import *\n",
    "from pkg.controller.combined_robot import *\n",
    "from pkg.geometry.builder.scene_builder import SceneBuilder\n",
    "from pkg.geometry.geometry import GeometryItem\n",
    "from pkg.geometry.geotype import GEOTYPE\n",
    "from pkg.detector.detector_interface import DetectionLevel\n",
    "from pkg.detector.multiICP.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Combined Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.detector.combined_detector import CombinedDetector\n",
    "from pkg.detector.multiICP.multiICP import MultiICP, MultiICP_Obj\n",
    "from pkg.detector.aruco.stereo import ArucoStereo\n",
    "from pkg.detector.aruco.marker_config import get_aruco_map\n",
    "from pkg.detector.camera.kinect import Kinect\n",
    "from pkg.detector.camera.realsense import RealSense\n",
    "from pkg.detector.detector_interface import DetectionLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = Kinect()\n",
    "rs = RealSense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create ArucoStereo instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device configuration: \n",
      "\tcolor_format: 3 \n",
      "\t(0:JPG, 1:NV12, 2:YUY2, 3:BGRA32)\n",
      "\n",
      "\tcolor_resolution: 5 \n",
      "\t(0:OFF, 1:720p, 2:1080p, 3:1440p, 4:1536p, 5:2160p, 6:3072p)\n",
      "\n",
      "\tdepth_mode: 3 \n",
      "\t(0:OFF, 1:NFOV_2X2BINNED, 2:NFOV_UNBINNED,3:WFOV_2X2BINNED, 4:WFOV_UNBINNED, 5:Passive IR)\n",
      "\n",
      "\tcamera_fps: 2 \n",
      "\t(0:5 FPS, 1:15 FPS, 2:30 FPS)\n",
      "\n",
      "\tsynchronized_images_only: False \n",
      "\t(True of False). Drop images if the color and depth are not synchronized\n",
      "\n",
      "\tdepth_delay_off_color_usec: 0 ms. \n",
      "\tDelay between the color image and the depth image\n",
      "\n",
      "\twired_sync_mode: 0\n",
      "\t(0:Standalone mode, 1:Master mode, 2:Subordinate mode)\n",
      "\n",
      "\tsubordinate_delay_off_master_usec: 0 ms.\n",
      "\tThe external synchronization timing.\n",
      "\n",
      "\tdisable_streaming_indicator: False \n",
      "\t(True or False). Streaming indicator automatically turns on when the color or depth camera's are in use.\n",
      "\n",
      "\n",
      "Start streaming\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[1.82983423e+03, 0.00000000e+00, 1.91572046e+03],\n",
       "         [0.00000000e+00, 1.82983423e+03, 1.09876086e+03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]),\n",
       "  array([ 7.09966481e-01, -2.73409390e+00,  1.45804870e-03, -3.24774766e-04,\n",
       "          1.44911301e+00,  5.84310412e-01, -2.56374550e+00,  1.38472950e+00]),\n",
       "  0.001),\n",
       " (array([[1.39560388e+03, 0.00000000e+00, 9.62751587e+02],\n",
       "         [0.00000000e+00, 1.39531934e+03, 5.47687012e+02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]),\n",
       "  array([0., 0., 0., 0., 0.]),\n",
       "  0.0010000000474974513),\n",
       " array([[ 0.80965704, -0.03687985, -0.5857434 ,  0.7164127 ],\n",
       "        [ 0.15020925,  0.9778048 ,  0.14606512, -0.15069216],\n",
       "        [ 0.5673559 , -0.20624672,  0.7972262 ,  0.05524582],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stereo = ArucoStereo(aruco_map=get_aruco_map(), \n",
    "                     camera_list=[kn, rs])\n",
    "stereo.initialize()\n",
    "\n",
    "time.sleep(1) # Let the camera have some time to get stable\n",
    "stereo.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create multiICP instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device configuration: \n",
      "\tcolor_format: 3 \n",
      "\t(0:JPG, 1:NV12, 2:YUY2, 3:BGRA32)\n",
      "\n",
      "\tcolor_resolution: 5 \n",
      "\t(0:OFF, 1:720p, 2:1080p, 3:1440p, 4:1536p, 5:2160p, 6:3072p)\n",
      "\n",
      "\tdepth_mode: 3 \n",
      "\t(0:OFF, 1:NFOV_2X2BINNED, 2:NFOV_UNBINNED,3:WFOV_2X2BINNED, 4:WFOV_UNBINNED, 5:Passive IR)\n",
      "\n",
      "\tcamera_fps: 2 \n",
      "\t(0:5 FPS, 1:15 FPS, 2:30 FPS)\n",
      "\n",
      "\tsynchronized_images_only: False \n",
      "\t(True of False). Drop images if the color and depth are not synchronized\n",
      "\n",
      "\tdepth_delay_off_color_usec: 0 ms. \n",
      "\tDelay between the color image and the depth image\n",
      "\n",
      "\twired_sync_mode: 0\n",
      "\t(0:Standalone mode, 1:Master mode, 2:Subordinate mode)\n",
      "\n",
      "\tsubordinate_delay_off_master_usec: 0 ms.\n",
      "\tThe external synchronization timing.\n",
      "\n",
      "\tdisable_streaming_indicator: False \n",
      "\t(True or False). Streaming indicator automatically turns on when the color or depth camera's are in use.\n",
      "\n",
      "\n",
      "Initialize Done\n"
     ]
    }
   ],
   "source": [
    "micp = MultiICP(kn)\n",
    "micp.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = CombinedDetector([stereo, micp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create SceneBuilder instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_builder = SceneBuilder.instance(detector=detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### set reference coordinate and viewpoint (by StereoAruco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_kn = stereo.ref_coord_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create geometry scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection command:\n",
      "indy0: False\n",
      "panda1: False\n",
      "name_mask is []\n",
      "\u001b[93m[WARN] CombinedRobot is not set: call set_config()\u001b[0m\n",
      "Unable to register with master node [http://localhost:11311]: master may not be running yet. Will keep trying.\n",
      "Please create a subscriber to the marker\n",
      "publication OK\n",
      "published: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Please create a subscriber to the marker\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 9.99947071e-01, -1.79811133e-04, -1.02850888e-02,\n",
       "          1.18164876e-02],\n",
       "        [ 7.58552179e-03, -6.62449121e-01,  7.49068499e-01,\n",
       "         -5.12544692e-01],\n",
       "        [-6.94803894e-03, -7.49106884e-01, -6.62412703e-01,\n",
       "          5.52709341e-01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crob = CombinedRobot(\n",
    "    robots_on_scene=[\n",
    "        RobotConfig(0, RobotType.indy7, ((0,-0.3,0), (0,0,0)), None),\n",
    "        RobotConfig(1, RobotType.panda, ((0,0.3,0), (0,0,0)), None)], \n",
    "    connection_list=[False, False])\n",
    "\n",
    "xyz_rpy_robots = scene_builder.detect_items(level_mask=[DetectionLevel.ROBOT])\n",
    "crob.update_robot_pos_dict(xyz_rpy_robots=xyz_rpy_robots)\n",
    "gscene = scene_builder.create_gscene(crob)\n",
    "gscene.show_pose(crob.home_pose)\n",
    "viewpoint = gscene.create_safe(gtype=GEOTYPE.SPHERE, name=\"viewpoint\", link_name=\"base_link\",\n",
    "                               dims=(0.05, 0.05, 0.02), center=T_kn[:3,3], rpy=Rot2rpy(T_kn[:3,:3]),\n",
    "                               color=(1, 0, 0, 0.3), display=True, fixed=True, collision=False)\n",
    "viewpoint.draw_traj_coords([crob.home_pose])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MultiICP configs\n",
    "* You have to make micp, hrule and grule for each object you want to detect\n",
    "\n",
    "* hrule means heuristic rule for special object which cannot be detected thorugh mask rcnn using COCO dataset\n",
    "\n",
    "* grule means initial guess(R,t) for ICP\n",
    "\n",
    "* Run shraed detector to detect object in color image\n",
    "\n",
    "##### (FYI) Running shared detector for object detection on bash - only when you want to keep detector server for multiple program runs\n",
    "```bash\n",
    "python3 /home/jhkim/Projects/rnb-planning/src/pkg/detector/multiICP/shared_detector.py --dims='(720,1280,3)'\n",
    "```\n",
    "\n",
    "#### Clearing shared detector channels if zombie memory remains\n",
    "```python\n",
    "from pkg.utils.shared_function import clear_channels_on, sa\n",
    "clear_channels_on(\"SharedDetector\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.utils.shared_function import clear_channels_on, sa\n",
    "clear_channels_on(\"SharedDetector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Data size too big for shared function: 165888000 - shared_fun.inference.shareddetector.0\n"
     ]
    }
   ],
   "source": [
    "from pkg.detector.multiICP.shared_detector import SharedDetectorGen\n",
    "sd = SharedDetectorGen(tuple(reversed(micp.dsize))+(3,))()\n",
    "sd.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_info_dict = get_obj_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object items you want to detect\n",
    "# heuristic rule items, Initial guess rule items\n",
    "# micp_cup = MultiICP_Obj(obj_info_dict[\"cup\"], None,\n",
    "#                         OffsetOnModelCoord(\"cup\", R=Rot_axis(1, np.pi/2), offset=None))\n",
    "\n",
    "micp_cup = MultiICP_Obj(obj_info_dict[\"cup\"], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "micp_dict = {\"cup\": micp_cup}\n",
    "micp.set_config(micp_dict, sd, crob, viewpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "micp.set_ICP_thres(thres_ICP=0.2, thres_front_ICP=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### detect_and_register()\n",
    "* Detect items in the field of view and register them to the GeometryScene\n",
    "* They will appear in the RVIZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_mask is []\n"
     ]
    }
   ],
   "source": [
    "gtem_dict = scene_builder.detect_and_register(level_mask=[DetectionLevel.ENVIRONMENT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_mask is ['cup']\n",
      "===== Detected : cup, 1 object(s) =====\n",
      "\n",
      "'cup' is not in gscene. Use multiple initial guess\n",
      "\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.430370e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.61023054  0.7914271   0.03552265 -0.1933166 ]\n",
      " [ 0.46400654 -0.32070993 -0.8257379  -0.10135425]\n",
      " [-0.64211892  0.5203732  -0.56293432  0.65773073]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=3.218465e-03, and correspondence_set size of 2433\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.56176719  0.82619168 -0.04271954 -0.18980471]\n",
      " [ 0.44443681 -0.34494145 -0.82673535 -0.08973249]\n",
      " [-0.69777763  0.44544664 -0.56096675  0.68162423]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.430370e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.61018844  0.79145849  0.0355471  -0.1933163 ]\n",
      " [ 0.46403283 -0.32066721 -0.82573971 -0.10135704]\n",
      " [-0.64213992  0.52035183 -0.56293013  0.65773279]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=3.218465e-03, and correspondence_set size of 2433\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.56176721  0.82619169 -0.04271954 -0.18980471]\n",
      " [ 0.4444368  -0.34494144 -0.82673534 -0.08973249]\n",
      " [-0.69777763  0.44544664 -0.56096676  0.68162423]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.430370e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.6101884   0.79145851  0.0355471  -0.1933163 ]\n",
      " [ 0.46403282 -0.32066723 -0.82573971 -0.10135704]\n",
      " [-0.6421399   0.52035184 -0.56293012  0.65773279]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=3.218464e-03, and correspondence_set size of 2433\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.56176717  0.82619171 -0.04271955 -0.18980471]\n",
      " [ 0.44443679 -0.34494146 -0.82673535 -0.08973249]\n",
      " [-0.69777761  0.44544666 -0.56096675  0.68162423]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.430370e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.6101884   0.7914585   0.03554711 -0.1933163 ]\n",
      " [ 0.46403282 -0.32066724 -0.82573971 -0.10135704]\n",
      " [-0.64213992  0.52035185 -0.56293013  0.65773279]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=3.218464e-03, and correspondence_set size of 2433\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.56176716  0.82619171 -0.04271954 -0.18980471]\n",
      " [ 0.44443679 -0.34494147 -0.82673534 -0.08973249]\n",
      " [-0.69777763  0.44544666 -0.56096676  0.68162423]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.613990e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[-0.9439668   0.31632479  0.0941561  -0.11874038]\n",
      " [ 0.18916562  0.7523347  -0.63103792 -0.15116904]\n",
      " [-0.27044985 -0.57786773 -0.77001672  0.69912062]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=7.347993e-03, and correspondence_set size of 2604\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[-0.97626658  0.21656446  0.00186348 -0.10804014]\n",
      " [ 0.14718664  0.66977598 -0.72782981 -0.13299371]\n",
      " [-0.15887019 -0.71028161 -0.68575527  0.71045425]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.464301e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.50028158 -0.8655486  -0.02332326 -0.11896357]\n",
      " [-0.44494911 -0.2338834  -0.86447606 -0.06419761]\n",
      " [ 0.74279116  0.44285912 -0.5021326   0.60228572]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=4.608502e-03, and correspondence_set size of 2211\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.48573735 -0.8728926  -0.04601902 -0.11767319]\n",
      " [-0.49561091 -0.23166095 -0.83708008 -0.0526796 ]\n",
      " [ 0.72002023  0.42940859 -0.54514144  0.62522745]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.464301e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.50028157 -0.86554863 -0.02332325 -0.11896357]\n",
      " [-0.44494909 -0.23388339 -0.86447606 -0.06419761]\n",
      " [ 0.74279112  0.44285913 -0.5021326   0.60228572]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=4.608503e-03, and correspondence_set size of 2211\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.48573734 -0.87289263 -0.04601901 -0.11767319]\n",
      " [-0.49561088 -0.23166094 -0.83708008 -0.0526796 ]\n",
      " [ 0.7200202   0.42940859 -0.54514143  0.62522745]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=1.464301e-02, and correspondence_set size of 9261\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.5002816  -0.86554864 -0.02332326 -0.11896357]\n",
      " [-0.44494908 -0.23388339 -0.86447606 -0.06419761]\n",
      " [ 0.74279113  0.44285912 -0.50213261  0.60228572]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=4.608503e-03, and correspondence_set size of 2211\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.48573736 -0.87289264 -0.04601901 -0.11767319]\n",
      " [-0.49561088 -0.23166093 -0.83708007 -0.0526796 ]\n",
      " [ 0.72002021  0.42940859 -0.54514144  0.62522745]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "('Lowest rmse', 0.003218464245405309)\n",
      "Found 6DoF pose of cup_1\n",
      "\u001b[93m[WARN] Vertices for mesh should be have center point (0,0,0). Auto adjusting.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gtem_dict = scene_builder.detect_and_register(level_mask=[DetectionLevel.MOVABLE], \n",
    "                                              visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
