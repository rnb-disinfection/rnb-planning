{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: 59522, 19215\n"
     ]
    }
   ],
   "source": [
    "from pkg.data_collecting.load_data import *\n",
    "\n",
    "JOINT_NUM = 13\n",
    "CONVERTED_PATH = CONVERTED_PATH_DEFAULT\n",
    "\n",
    "gtimer = GlobalTimer()\n",
    "# ## Load Global params\n",
    "DATASET_LIST = sorted(os.listdir(CONVERTED_PATH))\n",
    "trainset_num = None\n",
    "if trainset_num is not None:\n",
    "    TRAINSET_LIST =  DATASET_LIST[:trainset_num]\n",
    "    TESTSET_LIST = DATASET_LIST[trainset_num:]\n",
    "else:\n",
    "    TRAINSET_LIST = ['20201214-165211', '20201216-021416', '20201218-024611']\n",
    "    TESTSET_LIST = ['20201208-121454', '20201212-232318', '20201213-061207']\n",
    "data_loader = DataLoader(JOINT_NUM)\n",
    "train_set = data_loader.get_dataset_args(TRAINSET_LIST, JOINT_NUM)\n",
    "N_train = len(train_set)\n",
    "test_set = data_loader.get_dataset_args(TESTSET_LIST, JOINT_NUM)\n",
    "N_test = len(test_set)\n",
    "print(f'Train/Test: {N_train}, {N_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.training.model import *\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ResNetModelTP()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: logs/gradient_tape/20210110-211851\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logpath = os.path.join('logs','gradient_tape',current_time)\n",
    "train_log_dir = os.path.join(logpath, 'train')\n",
    "test_log_dir = os.path.join(logpath, 'test')\n",
    "model_log_dir = os.path.join(logpath, 'model_{}/')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "shutil.copy(os.path.join('.','pkg','training','model.py' ), logpath)\n",
    "print(f'Log path: {logpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": " No algorithm worked!\n\t [[node res_net_model_tp/res_net/conv_block3d_4/res1a_branch2b/Conv3D (defined at /home/tamp/Projects/tamp_etasl/eTaSL/pkg/training/model.py:114) ]] [Op:__inference_train_step_28666]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node res_net_model_tp/res_net/conv_block3d_4/res1a_branch2b/Conv3D:\n res_net_model_tp/res_net/conv_block3d_4/activation_24/IdentityN (defined at /home/tamp/Projects/tamp_etasl/eTaSL/pkg/training/model.py:112)\n\nFunction call stack:\ntrain_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-566809376f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparate_dat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mLOG_STEP\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m:  No algorithm worked!\n\t [[node res_net_model_tp/res_net/conv_block3d_4/res1a_branch2b/Conv3D (defined at /home/tamp/Projects/tamp_etasl/eTaSL/pkg/training/model.py:114) ]] [Op:__inference_train_step_28666]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node res_net_model_tp/res_net/conv_block3d_4/res1a_branch2b/Conv3D:\n res_net_model_tp/res_net/conv_block3d_4/activation_24/IdentityN (defined at /home/tamp/Projects/tamp_etasl/eTaSL/pkg/training/model.py:112)\n\nFunction call stack:\ntrain_step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_S = 0\n",
    "EPOCHS_E = 30\n",
    "BATCH_SIZE = 32\n",
    "LOG_STEP = 100\n",
    "\n",
    "for epoch in range(EPOCHS_S, EPOCHS_E):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    random.shuffle(train_set)\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in train_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m = data_loader.separate_dat(np.array(scene_batch, dtype=np.float32))\n",
    "            train_step([cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m], np.array(success_batch, dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in test_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m = data_loader.separate_dat(np.array(scene_batch, dtype=np.float32))\n",
    "            test_step([cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m], np.array(success_batch,dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "    model.save(model_log_dir.format(epoch + 1))\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "* input stage batch norm 없애보기 - 미미한 향상\n",
    "* dense depth 조정, add layer를 concat으로 - trainset 위주의 미미한 향상\n",
    "* input stage kernel size 1로, feature size down, dropout - 상당한 향성\n",
    "* dropout 증강 - 미미한 변화\n",
    "* stage0~5 last feature size 절반 - 무변화\n",
    "* box preconv 통합, feature size 추가 축소 - 성능 하락\n",
    "* box preconv 분리 - 성능 복구\n",
    "* feature size 절반, batch size 128 - 미미한 성능 하락\n",
    "* feature size 복구, batch size 64 - 성능 복구 ~ 77% (20201224-122148)\n",
    "* input non-batchnorm 확대 - 차이 없음 ~ 77% (20201224-165237)\n",
    "* 인풋 수정, input bn 복구 - 성능 하락 ~ 73% (20210106-150839) - 데이터 로딩 실수, 분리된 파일들에 기존의 내부 카운팅으로 중복 로드\n",
    "* 인풋 joint limit 상대값, 데이터 디버그 - 하락 상태, 큰 차이 없음 -74% (20210107-033852) \n",
    "* 인풋 피쳐만 절반 - 전혀 학습X (20210107-172454)\n",
    "* 전반적 피쳐 확대 - 전전과 비등 (20210107-230440)\n",
    "* elu/sigmoid 적용 - 수렴 속도만 빨라짐 (20210108-064925)\n",
    "* swish 적용 -  (20210108-202908)\n",
    "\n",
    "\n",
    "* input stage batch norm 더 없애보기\n",
    "* fix data 비주얼로 검증해보기\n",
    "* non-fix train 결과로 fixed 결과에 테스트 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train log\n",
    "* dense bn for end stage: saturated output\n",
    "* dense batch norm for each stage: 74% for train, saturated for test\n",
    "* dense linear for each stage: 73% for train, saturated for test\n",
    "* dense bn relu - dense linear for each stage: 74% for train, saturated for test\n",
    "* dense bn relu - dense linear for each stage (size down): 74% for train, saturated for test\n",
    "* separate input - dense bn relu - dense linear for each stage : 50epoch - 98.5% for train, max 74% for test\n",
    "* separate input - dense bn relu - dense linear for each stage : 60k data - 95% for train, 75% for test\n",
    "* 32 batch - OOM\n",
    "* dense^2 relu/sigmoid for each stage: \n",
    "* dense^2 relu/sigmoid x2 for each stage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX= np.zeros((5,5,5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX[:,:,:,:] = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX[0,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use_bias=True\n",
    "```\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 1, Loss: 0.6301059126853943, Accuracy: 67.04729461669922, Test Loss: 0.6941890120506287, Test Accuracy: 60.01933288574219\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 2, Loss: 0.5641522407531738, Accuracy: 70.55376434326172, Test Loss: 1.1243842840194702, Test Accuracy: 58.13466262817383\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 3, Loss: 0.4620269536972046, Accuracy: 77.59195709228516, Test Loss: 0.6866365671157837, Test Accuracy: 59.51997756958008\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 4, Loss: 0.43595388531684875, Accuracy: 79.04708862304688, Test Loss: 2.0774402618408203, Test Accuracy: 52.75450897216797\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 5, Loss: 0.4291463792324066, Accuracy: 80.04749298095703, Test Loss: 0.5646138787269592, Test Accuracy: 67.20361328125\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 6, Loss: 0.4223867654800415, Accuracy: 79.8807601928711, Test Loss: 0.84770268201828, Test Accuracy: 54.961341857910156\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-143930/model.h5/assets\n",
    "Epoch 7, Loss: 0.41381269693374634, Accuracy: 80.5881118774414, Test Loss: 1.0013036727905273, Test Accuracy: 56.2983283996582\n",
    "train step - 6500/19801  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use_bias=False - 큰 차이 없음\n",
    "```\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-161530/model.h5/assets\n",
    "Epoch 1, Loss: 0.6417393088340759, Accuracy: 66.24393463134766, Test Loss: 0.6525431871414185, Test Accuracy: 64.90013122558594\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-161530/model.h5/assets\n",
    "Epoch 2, Loss: 0.5980429649353027, Accuracy: 69.05315399169922, Test Loss: 0.6825580596923828, Test Accuracy: 55.91172409057617\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-161530/model.h5/assets\n",
    "Epoch 3, Loss: 0.5077642202377319, Accuracy: 75.03031158447266, Test Loss: 0.7368553280830383, Test Accuracy: 62.12950897216797\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-161530/model.h5/assets\n",
    "Epoch 4, Loss: 0.47730717062950134, Accuracy: 76.97049713134766, Test Loss: 1.066005825996399, Test Accuracy: 56.459407806396484\n",
    "INFO:tensorflow:Assets written to: logs/gradient_tape/20201221-161530/model.h5/assets\n",
    "Epoch 5, Loss: 0.4632640480995178, Accuracy: 78.0820541381836, Test Loss: 0.769783079624176, Test Accuracy: 62.580543518066406\n",
    "train step - 12800/19801     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use_biase=False, robot position sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
