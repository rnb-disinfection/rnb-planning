{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "dill._dill._reverse_typemap['ObjectType'] = object\n",
    "dill._dill._reverse_typemap[b'ListType'] = list\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class GlobalTimer:\n",
    "    def __init__(self, scale=1000, timeunit='ms'):\n",
    "        self.reset(scale, timeunit)\n",
    "        \n",
    "    def reset(self, scale=1000, timeunit='ms'):\n",
    "        self.scale = scale\n",
    "        self.timeunit = timeunit\n",
    "        self.name_list = []\n",
    "        self.ts_dict = {}\n",
    "        self.time_dict = collections.defaultdict(lambda: 0)\n",
    "        self.min_time_dict = collections.defaultdict(lambda: 1e10)\n",
    "        self.max_time_dict = collections.defaultdict(lambda: 0)\n",
    "        self.count_dict = collections.defaultdict(lambda: 0)\n",
    "        self.timelist_dict = collections.defaultdict(list)\n",
    "        self.switch(True)\n",
    "        \n",
    "    def switch(self, onoff):\n",
    "        self.__on = onoff\n",
    "    \n",
    "    def tic(self, name):\n",
    "        if self.__on:\n",
    "            if name not in self.name_list:\n",
    "                self.name_list.append(name)\n",
    "            self.ts_dict[name] = time.time()\n",
    "        \n",
    "    def toc(self, name, stack=False):\n",
    "        if self.__on:\n",
    "            dt = (time.time() - self.ts_dict[name]) * self.scale\n",
    "            self.time_dict[name] = self.time_dict[name] + dt\n",
    "            self.min_time_dict[name] = min(self.min_time_dict[name], dt)\n",
    "            self.max_time_dict[name] = max(self.max_time_dict[name], dt)\n",
    "            self.count_dict[name] = self.count_dict[name] + 1\n",
    "            if stack:\n",
    "                self.timelist_dict[name].append(dt)\n",
    "            return dt\n",
    "            \n",
    "    def toctic(self, name_toc, name_tic, stack=False):\n",
    "        dt = self.toc(name_toc, stack=stack)\n",
    "        self.tic(name_tic)\n",
    "        return dt\n",
    "        \n",
    "    def print_time_log(self, names=None):\n",
    "        if names is None:\n",
    "            names = self.name_list\n",
    "        for name in names:\n",
    "            print(\"{name}: \\t{tot_T} {timeunit}/{tot_C} = {per_T} {timeunit} ({minT}/{maxT})\\n\".format(\n",
    "                name=name, tot_T=np.round(np.sum(self.time_dict[name])), tot_C=self.count_dict[name], \n",
    "                per_T= np.round(np.sum(self.time_dict[name])/self.count_dict[name], 3),\n",
    "                timeunit=self.timeunit, minT=round(self.min_time_dict[name],3), maxT=round(self.max_time_dict[name],3)\n",
    "            ))\n",
    "            \n",
    "    def __str__(self):\n",
    "        strout = \"\" \n",
    "        names = self.name_list\n",
    "        for name in names:\n",
    "            strout += \"{name}: \\t{tot_T} {timeunit}/{tot_C} = {per_T} {timeunit} ({minT}/{maxT})\\n\".format(\n",
    "                name=name, tot_T=np.round(np.sum(self.time_dict[name])), tot_C=self.count_dict[name], \n",
    "                per_T= np.round(np.sum(self.time_dict[name])/self.count_dict[name], 3),\n",
    "                timeunit=self.timeunit, minT=round(self.min_time_dict[name],3), maxT=round(self.max_time_dict[name],3)\n",
    "            )\n",
    "        return strout\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "        return data\n",
    "\n",
    "def get_action_count(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION):\n",
    "    action_data_list = load_pickle(os.path.join(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION.replace(\"json\", \"pkl\")))\n",
    "    return len(action_data_list)\n",
    "\n",
    "def load_scene_data(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, idx_act, joint_num):\n",
    "\n",
    "    N_vtx_box = 3*8\n",
    "    N_mask_box = 1\n",
    "    N_joint_box = joint_num\n",
    "    N_label_box = N_vtx_box+N_mask_box+N_joint_box\n",
    "    N_vtx_cyl = 3*2+1\n",
    "    N_mask_cyl = 1\n",
    "    N_joint_cyl = joint_num\n",
    "    N_label_cyl = N_vtx_cyl+N_mask_cyl+N_joint_cyl\n",
    "    N_vtx_init = 3*8\n",
    "    N_mask_init = 1\n",
    "    N_joint_init = joint_num\n",
    "    N_label_init = N_vtx_init+N_mask_init+N_joint_init\n",
    "    N_vtx_goal = 3*8\n",
    "    N_mask_goal = 1\n",
    "    N_joint_goal = joint_num\n",
    "    N_label_goal = N_vtx_goal+N_mask_goal+N_joint_goal\n",
    "    N_joint_label = 6*joint_num\n",
    "    N_cell_label = N_label_box+N_label_cyl+N_label_init+N_label_goal + N_joint_label\n",
    "    N_BEGIN_CYL = N_vtx_box+N_mask_box+N_joint_box\n",
    "    N_BEGIN_INIT = N_BEGIN_CYL+N_vtx_cyl+N_mask_cyl+N_joint_cyl\n",
    "    N_BEGIN_GOAL = N_BEGIN_INIT+N_vtx_init+N_mask_init+N_joint_init\n",
    "\n",
    "    scene_pickle = load_pickle(os.path.join(CONVERTED_PATH, DATASET, WORLD, SCENE, \"scene.pkl\"))\n",
    "    scene_data = scene_pickle[b'scene_data']\n",
    "    ctem_names = scene_pickle[b'ctem_names']\n",
    "    ctem_cells = scene_pickle[b'ctem_cells']\n",
    "\n",
    "    action_data_list = load_pickle(os.path.join(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION.replace(\"json\", \"pkl\")))\n",
    "    act_dat = action_data_list[idx_act]\n",
    "    init_box_dat = act_dat[b'init_box_dat']\n",
    "    goal_box_dat = act_dat[b'goal_box_dat']\n",
    "    ctem_dat_list = act_dat[b'ctem_dat_list']\n",
    "    skey = int(act_dat[b'skey'])\n",
    "    success = act_dat[b'success']\n",
    "    ### put init, goal item data\n",
    "    cell, verts, chain = init_box_dat\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_INIT:N_BEGIN_INIT+N_vtx_init] = verts\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_INIT+N_vtx_init:N_BEGIN_INIT+N_vtx_init+N_mask_init] = 1\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_INIT+N_vtx_init+N_mask_init:N_BEGIN_INIT+N_vtx_init+N_mask_init+N_joint_init] = chain\n",
    "\n",
    "    cell, verts, chain = goal_box_dat\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_GOAL:N_BEGIN_GOAL+N_vtx_goal] = verts\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_GOAL+N_vtx_goal:N_BEGIN_GOAL+N_vtx_goal+N_mask_goal] = 1\n",
    "    scene_data[cell[0],cell[1],cell[2],N_BEGIN_GOAL+N_vtx_goal+N_mask_goal:N_BEGIN_GOAL+N_vtx_goal+N_mask_goal+N_joint_goal] = chain\n",
    "\n",
    "    ### add/replace collilsion object\n",
    "    for cname, ctype, cell, verts, chain in ctem_dat_list:\n",
    "        if ctype == b'BOX':\n",
    "            N_BEGIN_REP, N_vtx, N_mask, N_joint = 0, N_vtx_box, N_mask_box, N_joint_box\n",
    "        elif ctype == b'CYLINDER':\n",
    "            N_BEGIN_REP, N_vtx, N_mask, N_joint = N_BEGIN_CYL, N_vtx_cyl, N_mask_cyl, N_joint_cyl\n",
    "        else:\n",
    "            raise(RuntimeError(\"Non considered shape key\"))\n",
    "        scene_data[cell[0],cell[1],cell[2],N_BEGIN_REP:N_BEGIN_REP+N_vtx] = verts\n",
    "        scene_data[cell[0],cell[1],cell[2],N_BEGIN_REP+N_vtx:N_BEGIN_REP+N_vtx+N_mask] = 1\n",
    "        scene_data[cell[0],cell[1],cell[2],N_BEGIN_REP+N_vtx+N_mask:N_BEGIN_REP+N_vtx+N_mask+N_joint] = chain\n",
    "    return scene_data, success, skey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERTED_PATH = \"./data/converted\"\n",
    "SCENE_FILENAME = \"scene.pkl\"\n",
    "JOINT_NUM = 13\n",
    "gtimer = GlobalTimer()\n",
    "# ## Load Global params\n",
    "DATASET_LIST = sorted(filter(lambda x: x not in [\"backup\", \"converted\"], os.listdir(CONVERTED_PATH)))\n",
    "DATASET = DATASET_LIST[0]\n",
    "CURRENT_PATH = os.path.join(CONVERTED_PATH, DATASET)\n",
    "#Iterate world\n",
    "WORLD_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(CURRENT_PATH)))\n",
    "gtimer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_scene_data: \t6023.0 ms/6276 = 0.96 ms (0.659/6.082)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SCENE_TUPLE_LIST = []\n",
    "for WORLD in WORLD_LIST:\n",
    "    WORLD_PATH = os.path.join(CURRENT_PATH, WORLD)\n",
    "    # Iterate scene\n",
    "    SCENE_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(WORLD_PATH)))\n",
    "    for SCENE in SCENE_LIST:\n",
    "        SCENE_PATH = os.path.join(WORLD_PATH, SCENE)\n",
    "        ACTION_LIST = sorted(filter(lambda x: x != SCENE_FILENAME, os.listdir(SCENE_PATH)))\n",
    "        for ACTION in ACTION_LIST:\n",
    "            N_action = get_action_count(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION)\n",
    "            for i_act in range(N_action):\n",
    "                SCENE_TUPLE_LIST.append((CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM))\n",
    "                gtimer.tic(\"load_scene_data\")\n",
    "                scene_data, success, skey = load_scene_data(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM)\n",
    "                gtimer.toc(\"load_scene_data\")\n",
    "print(gtimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.7\n",
    "import random\n",
    "N_train  = int(len(SCENE_TUPLE_LIST)*TRAIN_RATIO)\n",
    "N_test = len(SCENE_TUPLE_LIST) - N_train\n",
    "train_set = SCENE_TUPLE_LIST[:N_train]\n",
    "test_set = SCENE_TUPLE_LIST[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPool3D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv3D(256, 3, activation='relu', padding='same') # 15\n",
    "        self.conv2 = Conv3D(256, 3, activation='relu', padding='same')\n",
    "        self.pool1 = MaxPool3D(pool_size=(3, 3, 3), strides=(2,2,2), padding='valid') # 7\n",
    "        self.conv3 = Conv3D(256, 3, activation='relu', padding='same')\n",
    "        self.conv4 = Conv3D(256, 3, activation='relu', padding='same')\n",
    "        self.pool2 = MaxPool3D(pool_size=(3, 3, 3), strides=(2,2,2), padding='valid') # 3\n",
    "        self.conv5 = Conv3D(256, 3, activation='relu', padding='same')\n",
    "        self.conv6 = Conv3D(256, 3, activation='relu', padding='same')\n",
    "        self.pool3 = MaxPool3D(pool_size=(3, 3, 3), strides=(2,2,2), padding='valid') # 1\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(256, activation='relu')\n",
    "        self.d2 = Dense(128, activation='relu')\n",
    "        self.d3 = Dense(64)\n",
    "        self.d4 = Dense(2, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        return self.d4(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6668033599853516, Accuracy: 64.71259307861328, Test Loss: 0.6591668128967285, Test Accuracy: 65.40948486328125\n",
      "Epoch 2, Loss: 0.6654521822929382, Accuracy: 64.78102111816406, Test Loss: 0.6591668128967285, Test Accuracy: 65.40948486328125\n",
      "Epoch 3, Loss: 0.6654521822929382, Accuracy: 64.78102111816406, Test Loss: 0.6591668128967285, Test Accuracy: 65.40948486328125\n",
      "Epoch 4, Loss: 0.6661364436149597, Accuracy: 64.71259307861328, Test Loss: 0.6591668128967285, Test Accuracy: 65.40948486328125\n",
      "Epoch 5, Loss: 0.6652240753173828, Accuracy: 64.8038330078125, Test Loss: 0.6591668128967285, Test Accuracy: 65.40948486328125\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LOG_STEP = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    random.shuffle(train_set)\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in train_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            train_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch, dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in test_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            test_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch,dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(success_batch,dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(np.array(scene_batch, dtype=np.float32), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = Dense(1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[3.248686e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest((tf.constant([[10]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27, 2), dtype=float32, numpy=\n",
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14766165614128113, Accuracy: 95.56000518798828, Test Loss: 0.06508717685937881, Test Accuracy: 97.79999542236328\n",
      "Epoch 2, Loss: 0.04511237516999245, Accuracy: 98.6500015258789, Test Loss: 0.05184425413608551, Test Accuracy: 98.25999450683594\n",
      "Epoch 3, Loss: 0.023387636989355087, Accuracy: 99.20832824707031, Test Loss: 0.058742888271808624, Test Accuracy: 98.18999481201172\n",
      "Epoch 4, Loss: 0.015396256931126118, Accuracy: 99.48999786376953, Test Loss: 0.04931311309337616, Test Accuracy: 98.48999786376953\n",
      "Epoch 5, Loss: 0.009212623350322247, Accuracy: 99.70832824707031, Test Loss: 0.05627923458814621, Test Accuracy: 98.40999603271484\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=uint8, numpy=\n",
       "array([3, 4, 6, 2, 3, 4, 8, 6, 4, 2, 6, 3, 9, 0, 1, 0, 8, 3, 6, 0, 8, 0,\n",
       "       2, 2, 2, 0, 2, 2, 1, 3, 3, 7], dtype=uint8)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
