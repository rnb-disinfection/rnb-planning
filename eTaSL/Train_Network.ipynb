{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_scene_data: \t5754.0 ms/6276 = 0.917 ms (0.637/3.808)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pkg.utils.utils_python3 import *\n",
    "\n",
    "CONVERTED_PATH = \"./data/converted\"\n",
    "SCENE_FILENAME = \"scene.pkl\"\n",
    "JOINT_NUM = 13\n",
    "gtimer = GlobalTimer()\n",
    "# ## Load Global params\n",
    "DATASET_LIST = sorted(filter(lambda x: x not in [\"backup\", \"converted\"], os.listdir(CONVERTED_PATH)))\n",
    "DATASET = DATASET_LIST[0]\n",
    "CURRENT_PATH = os.path.join(CONVERTED_PATH, DATASET)\n",
    "#Iterate world\n",
    "WORLD_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(CURRENT_PATH)))\n",
    "gtimer.reset()\n",
    "SCENE_TUPLE_LIST = []\n",
    "for WORLD in WORLD_LIST:\n",
    "    WORLD_PATH = os.path.join(CURRENT_PATH, WORLD)\n",
    "    # Iterate scene\n",
    "    SCENE_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(WORLD_PATH)))\n",
    "    for SCENE in SCENE_LIST:\n",
    "        SCENE_PATH = os.path.join(WORLD_PATH, SCENE)\n",
    "        ACTION_LIST = sorted(filter(lambda x: x != SCENE_FILENAME, os.listdir(SCENE_PATH)))\n",
    "        for ACTION in ACTION_LIST:\n",
    "            N_action = get_action_count(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION)\n",
    "            for i_act in range(N_action):\n",
    "                SCENE_TUPLE_LIST.append((CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM))\n",
    "                gtimer.tic(\"load_scene_data\")\n",
    "                scene_data, success, skey = load_scene_data(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM)\n",
    "                gtimer.toc(\"load_scene_data\")\n",
    "print(gtimer)\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "import random\n",
    "N_train  = int(len(SCENE_TUPLE_LIST)*TRAIN_RATIO)\n",
    "N_test = len(SCENE_TUPLE_LIST) - N_train\n",
    "train_set = SCENE_TUPLE_LIST[:N_train]\n",
    "test_set = SCENE_TUPLE_LIST[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers as KL\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class DenseBN:\n",
    "    def __init__(self, units, name, use_bias=True, activation=\"relu\"):\n",
    "        self.dense = KL.Dense(units, name='dense_' + name, use_bias=use_bias)\n",
    "        self.bn = KL.BatchNormalization(name='bn_' + name)\n",
    "        self.ac = KL.Activation(activation)\n",
    "        \n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.dense(x)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = self.ac(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class IdentityBlock:\n",
    "    def __init__(self, kernel_size, filters, stage, block,\n",
    "                 use_bias=True, ConvLayer=KL.Conv2D):\n",
    "        nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "        \n",
    "        self.conv1 = ConvLayer(nb_filter1, 1, name=conv_name_base + '2a',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn1 = KL.BatchNormalization(name=bn_name_base + '2a')\n",
    "        self.ac1 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv2 = ConvLayer(nb_filter2, kernel_size, padding=\"same\",\n",
    "                               name=conv_name_base + '2b', use_bias=use_bias)\n",
    "        self.bn2 = KL.BatchNormalization(name=bn_name_base + '2b')\n",
    "        self.ac2 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv3 = ConvLayer(nb_filter3, 1, name=conv_name_base + '2c',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn3 = KL.BatchNormalization(name=bn_name_base + '2c')\n",
    "        self.add = KL.Add()\n",
    "        self.ac3 = KL.Activation('relu', name='res' + str(stage) + block + '_out')\n",
    "        \n",
    "    def __call__(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.ac1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x = self.add([x, input_tensor])\n",
    "        x = self.ac3(x)\n",
    "        return x\n",
    "    \n",
    "class IdentityBlock3D(IdentityBlock):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(IdentityBlock3D, self).__init__(*args, ConvLayer=KL.Conv3D, **kwargs)\n",
    "    \n",
    "    \n",
    "\n",
    "class ConvBlock:\n",
    "    def __init__(self, kernel_size, filters, stage, block, strides=2,\n",
    "                 use_bias=True, ConvLayer=KL.Conv2D):\n",
    "        nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "        \n",
    "        self.conv1 = ConvLayer(nb_filter1, 1, strides=strides, \n",
    "                               name=conv_name_base + '2a', use_bias=use_bias)\n",
    "        self.bn1 = KL.BatchNormalization(name=bn_name_base + '2a')\n",
    "        self.ac1 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv2 = ConvLayer(nb_filter2, kernel_size, padding=\"same\",\n",
    "                               name=conv_name_base + '2b', use_bias=use_bias)\n",
    "        self.bn2 = KL.BatchNormalization(name=bn_name_base + '2b')\n",
    "        self.ac2 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv3 = ConvLayer(nb_filter3, 1, name=conv_name_base + '2c',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn3 = KL.BatchNormalization(name=bn_name_base + '2c')\n",
    "\n",
    "        self.convs = ConvLayer(nb_filter3, 1, strides=strides,\n",
    "                               name=conv_name_base + '1', use_bias=use_bias)\n",
    "        self.bns = KL.BatchNormalization(name=bn_name_base + '1')\n",
    "    \n",
    "        self.add = KL.Add()\n",
    "        self.ac3 = KL.Activation('relu', name='res' + str(stage) + block + '_out')\n",
    "        \n",
    "    def __call__(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.ac1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        \n",
    "        shortcut = self.convs(input_tensor)\n",
    "        shortcut = self.bns(shortcut, training=training)\n",
    "\n",
    "        x = self.add([x, shortcut])\n",
    "        x = self.ac3(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock3D(ConvBlock):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ConvBlock3D, self).__init__(*args, ConvLayer=KL.Conv3D, **kwargs)\n",
    "    \n",
    "\n",
    "class ResNet:\n",
    "    def __init__(self, architecture=\"resnet50\", stage1=None, stage2=[64, 64, 256], stage3=[128, 128, 512], \n",
    "                 stage4=[256, 256, 1024], stage5=None, \n",
    "                 ConvLayer=KL.Conv2D, ZeroPadding=KL.ZeroPadding2D, MaxPool=KL.MaxPooling2D, \n",
    "                 ConvBlock=ConvBlock, IdentityBlock=IdentityBlock):\n",
    "        \"\"\"Build a ResNet graph.\n",
    "            architecture: Can be resnet50 or resnet101\n",
    "            stage5: recommanded default values = [512, 512, 2048]. If None, stage5 of the network is not created\n",
    "        \"\"\"\n",
    "        assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "        self.stage1 = stage1 is not None\n",
    "        self.stage5 = stage5 is not None\n",
    "        \n",
    "        if self.stage1:\n",
    "            # Stage 1  output size = 1/1\n",
    "            self.cb1a = ConvBlock(3, stage1, stage=1, block='a', strides=1)\n",
    "            self.ib1b = IdentityBlock(3, stage2, stage=1, block='b')\n",
    "            self.ib1c = IdentityBlock(3, stage2, stage=1, block='c')\n",
    "            stride_2 = 2\n",
    "        else:\n",
    "            # Stage 1 original output size = 1/4\n",
    "            self.cv1 = ConvLayer(64, 7, strides=2, \n",
    "                                 name='conv1', padding=\"same\", use_bias=True)\n",
    "            self.bn1 = KL.BatchNormalization(name='bn_conv1')\n",
    "            self.ac1 = KL.Activation('relu')\n",
    "            self.mp1 = MaxPool(3, strides=2, padding=\"same\")\n",
    "            stride_2 = 1\n",
    "        \n",
    "        # Stage 2  output size = 1/stride_2\n",
    "        self.cb2a = ConvBlock(3, stage2, stage=2, block='a', strides=stride_2)\n",
    "        self.ib2b = IdentityBlock(3, stage2, stage=2, block='b')\n",
    "        self.ib2c = IdentityBlock(3, stage2, stage=2, block='c')\n",
    "        \n",
    "        # Stage 3  output size = 1/2\n",
    "        self.cb3a = ConvBlock(3, stage3, stage=3, block='a')\n",
    "        self.ib3b = IdentityBlock(3, stage3, stage=3, block='b')\n",
    "        self.ib3c = IdentityBlock(3, stage3, stage=3, block='c')\n",
    "        self.ib3d = IdentityBlock(3, stage3, stage=3, block='d')\n",
    "        \n",
    "        # Stage 4  output size = 1/2\n",
    "        self.cb4a = ConvBlock(3, stage4, stage=4, block='a')\n",
    "        block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]\n",
    "        self.ib4_list = []\n",
    "        for i in range(block_count):\n",
    "            self.ib4_list.append(\n",
    "                IdentityBlock(3, stage4, stage=4, block=chr(98 + i))\n",
    "            )\n",
    "            \n",
    "        # Stage 5  output size = 1/2\n",
    "        if self.stage5:\n",
    "            self.cb5a = ConvBlock(3, stage5, stage=5, block='a')\n",
    "            self.ib5b = IdentityBlock(3, stage5, stage=5, block='b')\n",
    "            self.ib5c = IdentityBlock(3, stage5, stage=5, block='c')\n",
    "\n",
    "    def __call__(self, input_image, training=False):\n",
    "        x = input_image\n",
    "        # Stage 1\n",
    "        if self.stage1:\n",
    "            x = self.cb1a(x, training=training)\n",
    "            x = self.ib1b(x, training=training)\n",
    "            C1 = x = self.ib1c(x, training=training)\n",
    "        else:\n",
    "            x = self.cv1(x)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = self.ac1(x)\n",
    "            C1 = x = self.mp1(x)\n",
    "        \n",
    "        # Stage 2\n",
    "        x = self.cb2a(x, training=training)\n",
    "        x = self.ib2b(x, training=training)\n",
    "        C2 = x = self.ib2c(x, training=training)\n",
    "        \n",
    "        # Stage 3\n",
    "        x = self.cb3a(x, training=training)\n",
    "        x = self.ib3b(x, training=training)\n",
    "        x = self.ib3c(x, training=training)\n",
    "        C3 = x = self.ib3d(x, training=training)\n",
    "        \n",
    "        # Stage 4\n",
    "        x = self.cb4a(x, training=training)\n",
    "        for ib4 in self.ib4_list:\n",
    "            x = ib4(x, training=training)\n",
    "        C4 = x\n",
    "            \n",
    "        # Stage 5\n",
    "        if self.stage5:\n",
    "            x = self.cb5a(x, training=training)\n",
    "            x = self.ib5b(x, training=training)\n",
    "            C5 = x = self.ib5c(x, training=training)\n",
    "            return [C1, C2, C3, C4, C5]\n",
    "        else:\n",
    "            return [C1, C2, C3, C4]\n",
    "            \n",
    "    \n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModelTP(Model):\n",
    "    def __init__(self):\n",
    "        super(ResNetModelTP, self).__init__()\n",
    "        self.resnet = ResNet(architecture=\"resnet50\", stage1=[256, 256, 512], \n",
    "                             stage2=[256, 256, 512], stage3=[512, 512, 1024], \n",
    "                             stage4=[512, 512, 1024], stage5=[512, 512, 1024], \n",
    "                             ConvLayer=KL.Conv3D, ZeroPadding=KL.ZeroPadding3D, MaxPool=KL.MaxPooling3D, \n",
    "                             ConvBlock=ConvBlock3D, IdentityBlock=IdentityBlock3D)\n",
    "        self.flatten = KL.Flatten()\n",
    "        self.gp_list = []\n",
    "        for _ in range(4):\n",
    "            self.gp_list.append(KL.GlobalMaxPool3D())\n",
    "        self.add1024 = KL.Add()\n",
    "        self.add512 = KL.Add()\n",
    "        self.dens1 = DenseBN(512, \"dens2\")\n",
    "        self.dens2 = DenseBN(256, \"dens3\")\n",
    "        self.dens3 = Dense(2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = self.resnet(x, training=training)\n",
    "        features_pool = []\n",
    "        for gp, ft in zip(self.gp_list features[:-1]):\n",
    "            features_pool.append(gp(ft))\n",
    "        x = self.flatten(features[-1])\n",
    "        x = self.add1024(features_pool[2:]+[x])\n",
    "        x = self.dens1(x, training=training)\n",
    "        x = self.add512(features_pool[:2]+[x])\n",
    "        x = self.dens2(x, training=training)\n",
    "        return self.dens3(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ResNetModelTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7347690463066101, Accuracy: 59.03284454345703, Test Loss: 0.665594756603241, Test Accuracy: 65.43803405761719\n",
      "Epoch 2, Loss: 0.6926104426383972, Accuracy: 60.561134338378906, Test Loss: 0.6775022745132446, Test Accuracy: 62.5\n",
      "Epoch 3, Loss: 0.6766912937164307, Accuracy: 62.614051818847656, Test Loss: 0.6804439425468445, Test Accuracy: 62.12607192993164\n",
      "Epoch 4, Loss: 0.6693775653839111, Accuracy: 62.18065643310547, Test Loss: 0.7040701508522034, Test Accuracy: 54.00640869140625\n",
      "Epoch 5, Loss: 0.6631377935409546, Accuracy: 62.750911712646484, Test Loss: 0.6714820265769958, Test Accuracy: 60.95085906982422\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "LOG_STEP = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    random.shuffle(train_set)\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in train_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            train_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch, dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in test_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            test_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch,dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(success_batch,dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(np.array(scene_batch, dtype=np.float32), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = Dense(1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.99995804]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest((tf.constant([[10]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27, 2), dtype=float32, numpy=\n",
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14230504631996155, Accuracy: 95.68333435058594, Test Loss: 0.05939216911792755, Test Accuracy: 97.91999816894531\n",
      "Epoch 2, Loss: 0.04611682891845703, Accuracy: 98.52166748046875, Test Loss: 0.050787363201379776, Test Accuracy: 98.18999481201172\n",
      "Epoch 3, Loss: 0.024921394884586334, Accuracy: 99.20832824707031, Test Loss: 0.0528109110891819, Test Accuracy: 98.23999786376953\n",
      "Epoch 4, Loss: 0.016222544014453888, Accuracy: 99.46666717529297, Test Loss: 0.05345192924141884, Test Accuracy: 98.5\n",
      "Epoch 5, Loss: 0.010034429840743542, Accuracy: 99.66666412353516, Test Loss: 0.05353892967104912, Test Accuracy: 98.41999816894531\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=uint8, numpy=\n",
       "array([3, 4, 6, 2, 3, 4, 8, 6, 4, 2, 6, 3, 9, 0, 1, 0, 8, 3, 6, 0, 8, 0,\n",
       "       2, 2, 2, 0, 2, 2, 1, 3, 3, 7], dtype=uint8)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
