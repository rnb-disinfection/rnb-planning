{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pkg.utils.utils_python3 import *\n",
    "import random\n",
    "\n",
    "CONVERTED_PATH = \"./data/converted\"\n",
    "SCENE_FILENAME = \"scene.pkl\"\n",
    "JOINT_NUM = 13\n",
    "gtimer = GlobalTimer()\n",
    "# ## Load Global params\n",
    "DATASET_LIST = sorted(os.listdir(CONVERTED_PATH))\n",
    "trainset_num = None\n",
    "if trainset_num is not None:\n",
    "    TRAINSET_LIST =  DATASET_LIST[:trainset_num]\n",
    "    TESTSET_LIST = DATASET_LIST[trainset_num:]\n",
    "else:\n",
    "    TRAINSET_LIST = ['20201212-232318', '20201213-061207', '20201214-165211']\n",
    "    TESTSET_LIST = ['20201208-121454']\n",
    "    \n",
    "# trainset\n",
    "SCENE_TUPLE_LIST = []\n",
    "for DATASET in TRAINSET_LIST:\n",
    "    CURRENT_PATH = os.path.join(CONVERTED_PATH, DATASET)\n",
    "    #Iterate world\n",
    "    WORLD_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(CURRENT_PATH)))\n",
    "    gtimer.reset()\n",
    "    for WORLD in WORLD_LIST:\n",
    "        WORLD_PATH = os.path.join(CURRENT_PATH, WORLD)\n",
    "        # Iterate scene\n",
    "        SCENE_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(WORLD_PATH)))\n",
    "        for SCENE in SCENE_LIST:\n",
    "            SCENE_PATH = os.path.join(WORLD_PATH, SCENE)\n",
    "            ACTION_LIST = sorted(filter(lambda x: x != SCENE_FILENAME, os.listdir(SCENE_PATH)))\n",
    "            for ACTION in ACTION_LIST:\n",
    "                N_action = get_action_count(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION)\n",
    "                for i_act in range(N_action):\n",
    "                    SCENE_TUPLE_LIST.append((CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM))\n",
    "                    #gtimer.tic(\"load_scene_data\")\n",
    "                    #scene_data, success, skey = load_scene_data(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM)\n",
    "                    #gtimer.toc(\"load_scene_data\")\n",
    "train_set = SCENE_TUPLE_LIST\n",
    "N_train = len(train_set)\n",
    "\n",
    "# trainset\n",
    "SCENE_TUPLE_LIST = []\n",
    "for DATASET in TESTSET_LIST:\n",
    "    CURRENT_PATH = os.path.join(CONVERTED_PATH, DATASET)\n",
    "    #Iterate world\n",
    "    WORLD_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(CURRENT_PATH)))\n",
    "    gtimer.reset()\n",
    "    for WORLD in WORLD_LIST:\n",
    "        WORLD_PATH = os.path.join(CURRENT_PATH, WORLD)\n",
    "        # Iterate scene\n",
    "        SCENE_LIST = sorted(filter(lambda x: not x.endswith(\".json\"), os.listdir(WORLD_PATH)))\n",
    "        for SCENE in SCENE_LIST:\n",
    "            SCENE_PATH = os.path.join(WORLD_PATH, SCENE)\n",
    "            ACTION_LIST = sorted(filter(lambda x: x != SCENE_FILENAME, os.listdir(SCENE_PATH)))\n",
    "            for ACTION in ACTION_LIST:\n",
    "                N_action = get_action_count(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION)\n",
    "                for i_act in range(N_action):\n",
    "                    SCENE_TUPLE_LIST.append((CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM))\n",
    "                    #gtimer.tic(\"load_scene_data\")\n",
    "                    #scene_data, success, skey = load_scene_data(CONVERTED_PATH, DATASET, WORLD, SCENE, ACTION, i_act, JOINT_NUM)\n",
    "                    #gtimer.toc(\"load_scene_data\")\n",
    "test_set = SCENE_TUPLE_LIST\n",
    "N_test = len(test_set)\n",
    "print(gtimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_num = 13\n",
    "N_vtx_box = 3*8\n",
    "N_mask_box = 1\n",
    "N_joint_box = joint_num\n",
    "N_label_box = N_vtx_box+N_mask_box+N_joint_box\n",
    "N_vtx_cyl = 3*2+1\n",
    "N_mask_cyl = 1\n",
    "N_joint_cyl = joint_num\n",
    "N_label_cyl = N_vtx_cyl+N_mask_cyl+N_joint_cyl\n",
    "N_vtx_init = 3*8\n",
    "N_mask_init = 1\n",
    "N_joint_init = joint_num\n",
    "N_label_init = N_vtx_init+N_mask_init+N_joint_init\n",
    "N_vtx_goal = 3*8\n",
    "N_mask_goal = 1\n",
    "N_joint_goal = joint_num\n",
    "N_label_goal = N_vtx_goal+N_mask_goal+N_joint_goal\n",
    "N_joint_label = 6*joint_num\n",
    "N_cell_label = N_label_box+N_label_cyl+N_label_init+N_label_goal + N_joint_label\n",
    "N_BEGIN_CYL = N_vtx_box+N_mask_box+N_joint_box\n",
    "N_BEGIN_INIT = N_BEGIN_CYL+N_vtx_cyl+N_mask_cyl+N_joint_cyl\n",
    "N_BEGIN_GOAL = N_BEGIN_INIT+N_vtx_init+N_mask_init+N_joint_init\n",
    "def separate_dat(scene_data):\n",
    "    cbox = scene_data[:,:,:,:, :N_vtx_box]\n",
    "    cbox_m = scene_data[:,:,:,:, N_vtx_box]\n",
    "    cbox_j = scene_data[:,:,:,:, N_vtx_box+1:N_vtx_box+1+N_joint_box]\n",
    "    ccyl = scene_data[:,:,:,:, N_BEGIN_CYL:N_BEGIN_CYL+N_vtx_cyl]\n",
    "    ccyl_m = scene_data[:,:,:,:, N_BEGIN_CYL+N_vtx_cyl]\n",
    "    ccyl_j = scene_data[:,:,:,:, N_BEGIN_CYL+N_vtx_cyl+1:N_BEGIN_CYL+N_vtx_cyl+1+N_joint_cyl]\n",
    "    ibox = scene_data[:,:,:,:, N_BEGIN_INIT:N_BEGIN_INIT+N_vtx_box]\n",
    "    ibox_m = scene_data[:,:,:,:, N_BEGIN_INIT+N_vtx_box]\n",
    "    ibox_j = scene_data[:,:,:,:, N_BEGIN_INIT+N_vtx_box+1:N_BEGIN_INIT+N_vtx_box+1+N_joint_init]\n",
    "    gbox = scene_data[:,:,:,:, N_BEGIN_GOAL:N_BEGIN_GOAL+N_vtx_box]\n",
    "    gbox_m = scene_data[:,:,:,:, N_BEGIN_GOAL+N_vtx_box]\n",
    "    gbox_j = scene_data[:,:,:,:, N_BEGIN_GOAL+N_vtx_box+1:N_BEGIN_GOAL+N_vtx_box+1+N_joint_goal]\n",
    "    joints = scene_data[:,:,:,:,-N_joint_label:]\n",
    "    joints = np.reshape(joints, scene_data.shape[:4]+(-1,6))\n",
    "    cbox = np.concatenate([cbox, \n",
    "                           (joints*np.expand_dims(cbox_j, axis=-1)).reshape(scene_data.shape[:4]+(-1,))], \n",
    "                          axis=-1)\n",
    "    ccyl = np.concatenate([ccyl, \n",
    "                           (joints*np.expand_dims(ccyl_j, axis=-1)).reshape(scene_data.shape[:4]+(-1,))], \n",
    "                          axis=-1)\n",
    "    ibox = np.concatenate([ibox, \n",
    "                           (joints*np.expand_dims(ibox_j, axis=-1)).reshape(scene_data.shape[:4]+(-1,))], \n",
    "                          axis=-1)\n",
    "    gbox = np.concatenate([gbox, \n",
    "                           (joints*np.expand_dims(gbox_j, axis=-1)).reshape(scene_data.shape[:4]+(-1,))], \n",
    "                          axis=-1)\n",
    "    return (cbox, np.expand_dims(cbox_m, axis=-1), \n",
    "            ccyl, np.expand_dims(ccyl_m, axis=-1), \n",
    "            ibox, np.expand_dims(ibox_m, axis=-1), \n",
    "            gbox, np.expand_dims(gbox_m, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers as KL\n",
    "from tensorflow.keras import Model\n",
    "import datetime\n",
    "\n",
    "class DenseBN(KL.Layer):\n",
    "    def __init__(self, units, name, use_bias=True, activation=\"relu\"):\n",
    "        super(DenseBN, self).__init__()\n",
    "        self.dense = KL.Dense(units, name='dense_' + name, use_bias=use_bias)\n",
    "        self.bn = KL.BatchNormalization(name='bn_' + name)\n",
    "        self.ac = KL.Activation(activation)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense(x)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = self.ac(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class IdentityBlock(KL.Layer):\n",
    "    def __init__(self, kernel_size, filters, stage, block,\n",
    "                 use_bias=True, ConvLayer=KL.Conv2D):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "        \n",
    "        self.conv1 = ConvLayer(nb_filter1, 1, name=conv_name_base + '2a',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn1 = KL.BatchNormalization(name=bn_name_base + '2a')\n",
    "        self.ac1 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv2 = ConvLayer(nb_filter2, kernel_size, padding=\"same\",\n",
    "                               name=conv_name_base + '2b', use_bias=use_bias)\n",
    "        self.bn2 = KL.BatchNormalization(name=bn_name_base + '2b')\n",
    "        self.ac2 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv3 = ConvLayer(nb_filter3, 1, name=conv_name_base + '2c',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn3 = KL.BatchNormalization(name=bn_name_base + '2c')\n",
    "        self.add = KL.Add()\n",
    "        self.ac3 = KL.Activation('relu', name='res' + str(stage) + block + '_out')\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.ac1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x = self.add([x, input_tensor])\n",
    "        x = self.ac3(x)\n",
    "        return x\n",
    "    \n",
    "class IdentityBlock3D(IdentityBlock):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(IdentityBlock3D, self).__init__(*args, ConvLayer=KL.Conv3D, **kwargs)\n",
    "    \n",
    "    \n",
    "\n",
    "class ConvBlock(KL.Layer):\n",
    "    def __init__(self, kernel_size, filters, stage, block, strides=2,\n",
    "                 use_bias=True, ConvLayer=KL.Conv2D):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "        \n",
    "        self.conv1 = ConvLayer(nb_filter1, 1, strides=strides, \n",
    "                               name=conv_name_base + '2a', use_bias=use_bias)\n",
    "        self.bn1 = KL.BatchNormalization(name=bn_name_base + '2a')\n",
    "        self.ac1 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv2 = ConvLayer(nb_filter2, kernel_size, padding=\"same\",\n",
    "                               name=conv_name_base + '2b', use_bias=use_bias)\n",
    "        self.bn2 = KL.BatchNormalization(name=bn_name_base + '2b')\n",
    "        self.ac2 = KL.Activation('relu')\n",
    "        \n",
    "        self.conv3 = ConvLayer(nb_filter3, 1, name=conv_name_base + '2c',\n",
    "                               use_bias=use_bias)\n",
    "        self.bn3 = KL.BatchNormalization(name=bn_name_base + '2c')\n",
    "\n",
    "        self.convs = ConvLayer(nb_filter3, 1, strides=strides,\n",
    "                               name=conv_name_base + '1', use_bias=use_bias)\n",
    "        self.bns = KL.BatchNormalization(name=bn_name_base + '1')\n",
    "    \n",
    "        self.add = KL.Add()\n",
    "        self.ac3 = KL.Activation('relu', name='res' + str(stage) + block + '_out')\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.ac1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        \n",
    "        shortcut = self.convs(input_tensor)\n",
    "        shortcut = self.bns(shortcut, training=training)\n",
    "\n",
    "        x = self.add([x, shortcut])\n",
    "        x = self.ac3(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock3D(ConvBlock):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ConvBlock3D, self).__init__(*args, ConvLayer=KL.Conv3D, **kwargs)\n",
    "    \n",
    "\n",
    "class ResNet(KL.Layer):\n",
    "    def __init__(self, architecture=\"resnet50\", stage0=None, stage1=None, stage2=[64, 64, 256], stage3=[128, 128, 512], \n",
    "                 stage4=[256, 256, 1024], stage5=None, \n",
    "                 ConvLayer=KL.Conv2D, ZeroPadding=KL.ZeroPadding2D, MaxPool=KL.MaxPooling2D, \n",
    "                 ConvBlock=ConvBlock, IdentityBlock=IdentityBlock, input_size=(15,15,15), joint_num=13, batch_size=16):\n",
    "        super(ResNet, self).__init__()\n",
    "        \"\"\"Build a ResNet graph.\n",
    "            architecture: Can be resnet50 or resnet101\n",
    "            stage5: recommanded default values = [512, 512, 2048]. If None, stage5 of the network is not created\n",
    "        \"\"\"\n",
    "        assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "        self.stage1 = stage1 is not None\n",
    "        self.stage5 = stage5 is not None\n",
    "        self.stage0 = stage0 is not None\n",
    "        self.input_size = input_size\n",
    "        self.joint_num = joint_num\n",
    "        self.batch_size = batch_size\n",
    "        if self.stage0:\n",
    "            self.pre_boxa = ConvBlock(3, stage0, stage=0, block='ba', strides=1)\n",
    "            self.pre_boxb = IdentityBlock(3, stage0, stage=0, block='bb')\n",
    "            self.pre_boxc = IdentityBlock(3, stage0, stage=0, block='bc')\n",
    "            self.pre_cyla = ConvBlock(3, stage0, stage=0, block='ca', strides=1)\n",
    "            self.pre_cylb = IdentityBlock(3, stage0, stage=0, block='cb')\n",
    "            self.pre_cylc = IdentityBlock(3, stage0, stage=0, block='cc')\n",
    "            self.pre_inia = ConvBlock(3, stage0, stage=0, block='ia', strides=1)\n",
    "            self.pre_inib = IdentityBlock(3, stage0, stage=0, block='ib')\n",
    "            self.pre_inic = IdentityBlock(3, stage0, stage=0, block='ic')\n",
    "            self.pre_gola = ConvBlock(3, stage0, stage=0, block='ga', strides=1)\n",
    "            self.pre_golb = IdentityBlock(3, stage0, stage=0, block='gb')\n",
    "            self.pre_golc = IdentityBlock(3, stage0, stage=0, block='gc')\n",
    "            self.concat = KL.Concatenate()\n",
    "        \n",
    "        if self.stage1:\n",
    "            # Stage 1  output size = 1/1\n",
    "            self.cb1a = ConvBlock(3, stage1, stage=1, block='a', strides=1)\n",
    "            self.ib1b = IdentityBlock(3, stage1, stage=1, block='b')\n",
    "            self.ib1c = IdentityBlock(3, stage1, stage=1, block='c')\n",
    "            stride_2 = 2\n",
    "        else:\n",
    "            # Stage 1 original output size = 1/4\n",
    "            self.cv1 = ConvLayer(64, 7, strides=2, \n",
    "                                 name='conv1', padding=\"same\", use_bias=True)\n",
    "            self.bn1 = KL.BatchNormalization(name='bn_conv1')\n",
    "            self.ac1 = KL.Activation('relu')\n",
    "            self.mp1 = MaxPool(3, strides=2, padding=\"same\")\n",
    "            stride_2 = 1\n",
    "        \n",
    "        # Stage 2  output size = 1/stride_2\n",
    "        self.cb2a = ConvBlock(3, stage2, stage=2, block='a', strides=stride_2)\n",
    "        self.ib2b = IdentityBlock(3, stage2, stage=2, block='b')\n",
    "        self.ib2c = IdentityBlock(3, stage2, stage=2, block='c')\n",
    "        \n",
    "        # Stage 3  output size = 1/2\n",
    "        self.cb3a = ConvBlock(3, stage3, stage=3, block='a')\n",
    "        self.ib3b = IdentityBlock(3, stage3, stage=3, block='b')\n",
    "        self.ib3c = IdentityBlock(3, stage3, stage=3, block='c')\n",
    "        self.ib3d = IdentityBlock(3, stage3, stage=3, block='d')\n",
    "        \n",
    "        # Stage 4  output size = 1/2\n",
    "        self.cb4a = ConvBlock(3, stage4, stage=4, block='a')\n",
    "        block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture]\n",
    "        self.ib4_list = []\n",
    "        for i in range(block_count):\n",
    "            self.ib4_list.append(\n",
    "                IdentityBlock(3, stage4, stage=4, block=chr(98 + i))\n",
    "            )\n",
    "            \n",
    "        # Stage 5  output size = 1/2\n",
    "        if self.stage5:\n",
    "            self.cb5a = ConvBlock(3, stage5, stage=5, block='a')\n",
    "            self.ib5b = IdentityBlock(3, stage5, stage=5, block='b')\n",
    "            self.ib5c = IdentityBlock(3, stage5, stage=5, block='c')\n",
    "\n",
    "    def call(self, input_image, training=False):\n",
    "        if self.stage0:\n",
    "            cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m = input_image\n",
    "            cbox = self.pre_boxa(cbox, training=training)\n",
    "            cbox = self.pre_boxb(cbox, training=training)\n",
    "            cbox = self.pre_boxc(cbox, training=training)*cbox_m\n",
    "            ccyl = self.pre_cyla(ccyl, training=training)\n",
    "            ccyl = self.pre_cylb(ccyl, training=training)\n",
    "            ccyl = self.pre_cylc(ccyl, training=training)*ccyl_m\n",
    "            ibox = self.pre_inia(ibox, training=training)\n",
    "            ibox = self.pre_inib(ibox, training=training)\n",
    "            ibox = self.pre_inic(ibox, training=training)*ibox_m\n",
    "            gbox = self.pre_gola(gbox, training=training)\n",
    "            gbox = self.pre_golb(gbox, training=training)\n",
    "            gbox = self.pre_golc(gbox, training=training)*gbox_m\n",
    "            x = self.concat([cbox, ccyl, ibox, gbox])\n",
    "        else:\n",
    "            x = input_image\n",
    "        # Stage 1\n",
    "        if self.stage1:\n",
    "            x = self.cb1a(x, training=training)\n",
    "            x = self.ib1b(x, training=training)\n",
    "            C1 = x = self.ib1c(x, training=training)\n",
    "        else:\n",
    "            x = self.cv1(x)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = self.ac1(x)\n",
    "            C1 = x = self.mp1(x)\n",
    "        \n",
    "        # Stage 2\n",
    "        x = self.cb2a(x, training=training)\n",
    "        x = self.ib2b(x, training=training)\n",
    "        C2 = x = self.ib2c(x, training=training)\n",
    "        \n",
    "        # Stage 3\n",
    "        x = self.cb3a(x, training=training)\n",
    "        x = self.ib3b(x, training=training)\n",
    "        x = self.ib3c(x, training=training)\n",
    "        C3 = x = self.ib3d(x, training=training)\n",
    "        \n",
    "        # Stage 4\n",
    "        x = self.cb4a(x, training=training)\n",
    "        for ib4 in self.ib4_list:\n",
    "            x = ib4(x, training=training)\n",
    "        C4 = x\n",
    "            \n",
    "        # Stage 5\n",
    "        if self.stage5:\n",
    "            x = self.cb5a(x, training=training)\n",
    "            x = self.ib5b(x, training=training)\n",
    "            C5 = x = self.ib5c(x, training=training)\n",
    "            return [C1, C2, C3, C4, C5]\n",
    "        else:\n",
    "            return [C1, C2, C3, C4]\n",
    "            \n",
    "    \n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModelTP(Model):\n",
    "    def __init__(self, \n",
    "                 ConvLayer=KL.Conv3D, ZeroPadding=KL.ZeroPadding3D, MaxPool=KL.GlobalMaxPool3D, \n",
    "                 ConvBlock=ConvBlock3D, IdentityBlock=IdentityBlock3D):\n",
    "        super(ResNetModelTP, self).__init__()\n",
    "        self.resnet = ResNet(architecture=\"resnet50\", \n",
    "                             stage0 = [64,64,128], stage1=[256, 256, 512], \n",
    "                             stage2=[256, 256, 512], stage3=[256, 256, 512], \n",
    "                             stage4=[256, 256, 512], stage5=[256, 256, 512], \n",
    "                             ConvLayer=ConvLayer, ZeroPadding=ZeroPadding, MaxPool=MaxPool, \n",
    "                             ConvBlock=ConvBlock, IdentityBlock=IdentityBlock)\n",
    "        self.gp1 = MaxPool()\n",
    "        self.gp2 = MaxPool()\n",
    "        self.gp3 = MaxPool()\n",
    "        self.gp4 = MaxPool()\n",
    "        self.flatten = KL.Flatten()\n",
    "        self.dens11 = DenseBN(512, \"dens11\")\n",
    "        self.dens21 = DenseBN(512, \"dens21\")\n",
    "        self.dens31 = DenseBN(512, \"dens31\")\n",
    "        self.dens41 = DenseBN(512, \"dens41\")\n",
    "        self.dens51 = DenseBN(512, \"dens51\")\n",
    "        self.dens12 = KL.Dense(256) # DenseBN(512, \"dens1\")\n",
    "        self.dens22 = KL.Dense(256) # DenseBN(512, \"dens2\")\n",
    "        self.dens32 = KL.Dense(256) # DenseBN(512, \"dens3\")\n",
    "        self.dens42 = KL.Dense(256) # DenseBN(512, \"dens4\")\n",
    "        self.dens52 = KL.Dense(256) # DenseBN(512, \"dens5\")\n",
    "        self.add = KL.Add()\n",
    "        self.dens_int1 = DenseBN(128, \"dens_int1\")\n",
    "        self.dens_int2 = DenseBN(64, \"dens_int2\")\n",
    "        self.dens_out = KL.Dense(2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        C1, C2, C3, C4, C5 = self.resnet(x, training=training)\n",
    "        f1 = self.dens11(self.gp1(C1), training=training)\n",
    "        f1 = self.dens12(f1, training=training)\n",
    "        f2 = self.dens21(self.gp2(C2), training=training)\n",
    "        f2 = self.dens22(f2, training=training)\n",
    "        f3 = self.dens31(self.gp3(C3), training=training)\n",
    "        f3 = self.dens32(f3, training=training)\n",
    "        f4 = self.dens41(self.gp4(C4), training=training)\n",
    "        f4 = self.dens42(f4, training=training)\n",
    "        f5 = self.dens51(self.flatten(C5), training=training)\n",
    "        f5 = self.dens52(f5, training=training)\n",
    "        x = self.add([f1,f2,f3,f4,f5])\n",
    "        # x = f5\n",
    "        x = self.dens_int1(x, training=training)\n",
    "        x = self.dens_int2(x, training=training)\n",
    "        x = self.dens_out(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ResNetModelTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6050356030464172, Accuracy: 69.98587799072266, Test Loss: 0.5871066451072693, Test Accuracy: 71.10969543457031\n",
      "Epoch 2, Loss: 0.5882013440132141, Accuracy: 71.01309967041016, Test Loss: 0.6037904024124146, Test Accuracy: 70.67921447753906\n",
      "Epoch 3, Loss: 0.5776786208152771, Accuracy: 71.19286346435547, Test Loss: 0.5792039632797241, Test Accuracy: 72.1141586303711\n",
      "Epoch 4, Loss: 0.538532018661499, Accuracy: 73.40460205078125, Test Loss: 0.5715247392654419, Test Accuracy: 68.78189086914062\n",
      "Epoch 5, Loss: 0.5208334922790527, Accuracy: 74.88443756103516, Test Loss: 0.5665736794471741, Test Accuracy: 72.14604949951172\n",
      "Epoch 6, Loss: 0.510137677192688, Accuracy: 75.10592651367188, Test Loss: 0.5318417549133301, Test Accuracy: 75.03189086914062\n",
      "Epoch 7, Loss: 0.498340368270874, Accuracy: 75.8089370727539, Test Loss: 0.6081823110580444, Test Accuracy: 66.80484771728516\n",
      "Epoch 8, Loss: 0.4856240749359131, Accuracy: 76.60503387451172, Test Loss: 0.5485255122184753, Test Accuracy: 73.32588958740234\n",
      "Epoch 9, Loss: 0.47399091720581055, Accuracy: 77.32730102539062, Test Loss: 0.498191237449646, Test Accuracy: 76.22767639160156\n",
      "Epoch 10, Loss: 0.4623279571533203, Accuracy: 77.9821548461914, Test Loss: 0.5016804933547974, Test Accuracy: 74.64923095703125\n",
      "Epoch 11, Loss: 0.4495030343532562, Accuracy: 78.81034851074219, Test Loss: 0.4969269931316376, Test Accuracy: 75.57398223876953\n",
      "Epoch 12, Loss: 0.4365924596786499, Accuracy: 79.60323333740234, Test Loss: 0.4877016246318817, Test Accuracy: 75.90880584716797\n",
      "Epoch 13, Loss: 0.4219365119934082, Accuracy: 80.4089584350586, Test Loss: 0.5519115328788757, Test Accuracy: 75.33482360839844\n",
      "Epoch 14, Loss: 0.40619322657585144, Accuracy: 81.06702423095703, Test Loss: 0.5314342975616455, Test Accuracy: 75.70153045654297\n",
      "Epoch 15, Loss: 0.39533182978630066, Accuracy: 81.83423614501953, Test Loss: 0.511133074760437, Test Accuracy: 74.85650634765625\n",
      "Epoch 16, Loss: 0.3796197175979614, Accuracy: 82.75552368164062, Test Loss: 0.9140141010284424, Test Accuracy: 69.72257232666016\n",
      "Epoch 17, Loss: 0.36571866273880005, Accuracy: 83.37185668945312, Test Loss: 0.5913150310516357, Test Accuracy: 70.64732360839844\n",
      "Epoch 18, Loss: 0.35680434107780457, Accuracy: 83.84373474121094, Test Loss: 0.551613986492157, Test Accuracy: 72.27359771728516\n",
      "Epoch 19, Loss: 0.34366557002067566, Accuracy: 84.6976089477539, Test Loss: 0.5999028086662292, Test Accuracy: 75.43048095703125\n",
      "Epoch 20, Loss: 0.332492470741272, Accuracy: 85.36851501464844, Test Loss: 0.6660590171813965, Test Accuracy: 70.82270050048828\n",
      "Epoch 21, Loss: 0.32153502106666565, Accuracy: 85.60926818847656, Test Loss: 0.6308848261833191, Test Accuracy: 75.62181091308594\n",
      "Epoch 22, Loss: 0.30881020426750183, Accuracy: 86.3668441772461, Test Loss: 0.6088696122169495, Test Accuracy: 73.56505584716797\n",
      "Epoch 23, Loss: 0.2979620397090912, Accuracy: 86.77452087402344, Test Loss: 0.7004223465919495, Test Accuracy: 73.7244873046875\n",
      "Epoch 24, Loss: 0.28602540493011475, Accuracy: 87.47110748291016, Test Loss: 0.6067394614219666, Test Accuracy: 73.7244873046875\n",
      "Epoch 25, Loss: 0.27682286500930786, Accuracy: 87.6861801147461, Test Loss: 0.6622523069381714, Test Accuracy: 74.79273223876953\n",
      "Epoch 26, Loss: 0.2681632936000824, Accuracy: 88.44376373291016, Test Loss: 0.665165901184082, Test Accuracy: 73.21428680419922\n",
      "Epoch 27, Loss: 0.2562435269355774, Accuracy: 89.10503387451172, Test Loss: 1.1907109022140503, Test Accuracy: 72.19387817382812\n",
      "Epoch 28, Loss: 0.24945810437202454, Accuracy: 89.25590515136719, Test Loss: 0.7763983011245728, Test Accuracy: 73.53316497802734\n",
      "Epoch 29, Loss: 0.2402123510837555, Accuracy: 89.70531463623047, Test Loss: 0.8400248885154724, Test Accuracy: 69.97767639160156\n",
      "Epoch 30, Loss: 0.22978267073631287, Accuracy: 90.28312683105469, Test Loss: 0.7845519185066223, Test Accuracy: 72.79974365234375\n",
      "Epoch 31, Loss: 0.2251918911933899, Accuracy: 90.29596710205078, Test Loss: 0.7932900190353394, Test Accuracy: 74.55357360839844\n",
      "Epoch 32, Loss: 0.21047919988632202, Accuracy: 90.91871643066406, Test Loss: 0.7945733070373535, Test Accuracy: 74.10713958740234\n",
      "Epoch 33, Loss: 0.20610454678535461, Accuracy: 91.11774444580078, Test Loss: 0.8373076915740967, Test Accuracy: 73.4375\n",
      "Epoch 34, Loss: 0.19874916970729828, Accuracy: 91.54789733886719, Test Loss: 0.8205922245979309, Test Accuracy: 73.62882232666016\n",
      "Epoch 35, Loss: 0.19004669785499573, Accuracy: 91.93631744384766, Test Loss: 0.9505768418312073, Test Accuracy: 74.18685913085938\n",
      "Epoch 36, Loss: 0.17830601334571838, Accuracy: 92.50770568847656, Test Loss: 0.8636906147003174, Test Accuracy: 74.10713958740234\n",
      "Epoch 37, Loss: 0.1711278110742569, Accuracy: 92.81266021728516, Test Loss: 0.9245458841323853, Test Accuracy: 74.66517639160156\n",
      "Epoch 38, Loss: 0.16075876355171204, Accuracy: 93.28132629394531, Test Loss: 1.2248109579086304, Test Accuracy: 73.16645050048828\n",
      "Epoch 39, Loss: 0.15628387033939362, Accuracy: 93.41935729980469, Test Loss: 1.5326608419418335, Test Accuracy: 72.03443908691406\n",
      "Epoch 40, Loss: 0.1463482826948166, Accuracy: 93.93297576904297, Test Loss: 0.9433951377868652, Test Accuracy: 73.50127410888672\n",
      "Epoch 41, Loss: 0.1449904441833496, Accuracy: 93.92013549804688, Test Loss: 1.0161241292953491, Test Accuracy: 72.78380584716797\n",
      "Epoch 42, Loss: 0.1368366926908493, Accuracy: 94.33423614501953, Test Loss: 0.9543569684028625, Test Accuracy: 72.54463958740234\n",
      "Epoch 43, Loss: 0.12949633598327637, Accuracy: 94.62955474853516, Test Loss: 1.2736902236938477, Test Accuracy: 73.16645050048828\n",
      "Epoch 44, Loss: 0.13006830215454102, Accuracy: 94.72907257080078, Test Loss: 0.9776161313056946, Test Accuracy: 72.25765228271484\n",
      "Epoch 45, Loss: 0.1204260066151619, Accuracy: 95.13995361328125, Test Loss: 1.1380964517593384, Test Accuracy: 72.35331726074219\n",
      "Epoch 46, Loss: 0.11950196325778961, Accuracy: 95.17205810546875, Test Loss: 1.0464051961898804, Test Accuracy: 73.89987182617188\n",
      "train step - 22300/31166        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-282f22ffb8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_dat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccyl_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mibox_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbox_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mLOG_STEP\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS_S = 0\n",
    "EPOCHS_E = 80\n",
    "BATCH_SIZE = 16\n",
    "LOG_STEP = 100\n",
    "\n",
    "for epoch in range(EPOCHS_S, EPOCHS_E):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    random.shuffle(train_set)\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in train_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m = separate_dat(np.array(scene_batch, dtype=np.float32))\n",
    "            train_step([cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m], np.array(success_batch, dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for scene_tuple in test_set:\n",
    "        i_step += 1\n",
    "        scene_data, success, skey = load_scene_data(*scene_tuple)\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m = separate_dat(np.array(scene_batch, dtype=np.float32))\n",
    "            test_step([cbox, cbox_m, ccyl, ccyl_m, ibox, ibox_m, gbox, gbox_m], np.array(success_batch,dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train log\n",
    "* dense bn for end stage: saturated output\n",
    "* dense batch norm for each stage: 74% for train, saturated for test\n",
    "* dense linear for each stage: 73% for train, saturated for test\n",
    "* dense bn relu - dense linear for each stage: 74% for train, saturated for test\n",
    "* dense bn relu - dense linear for each stage (size down): 74% for train, saturated for test\n",
    "* separate input - dense bn relu - dense linear for each stage : 50epoch - 98.5% for train, max 74% for test\n",
    "* dense^2 relu/sigmoid for each stage: \n",
    "* dense^2 relu/sigmoid x2 for each stage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ResNetModelTest(Model):\n",
    "    def __init__(self):\n",
    "        super(ResNetModelTest, self).__init__()\n",
    "        self.resnet = ResNet(architecture=\"resnet50\", stage1=[256, 256, 512], \n",
    "                             stage2=[256, 256, 512], stage3=[512, 512, 1024], \n",
    "                             stage4=[512, 512, 1024], stage5=[512, 512, 1024])\n",
    "        self.gp1 = KL.GlobalMaxPool2D()\n",
    "        self.gp2 = KL.GlobalMaxPool2D()\n",
    "        self.gp3 = KL.GlobalMaxPool2D()\n",
    "        self.gp4 = KL.GlobalMaxPool2D()\n",
    "        self.flatten = KL.Flatten()\n",
    "        self.dens1 = DenseBN(512, \"dens1\")\n",
    "        self.dens2 = DenseBN(512, \"dens2\")\n",
    "        self.dens3 = DenseBN(512, \"dens3\")\n",
    "        self.dens4 = DenseBN(512, \"dens4\")\n",
    "        self.dens5 = DenseBN(512, \"dens5\")\n",
    "        self.add = KL.Add()\n",
    "        self.dens_int = DenseBN(128, \"dens_int\")\n",
    "        self.dens_out = KL.Dense(2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        C1, C2, C3, C4, C5 = self.resnet(x, training=training)\n",
    "        # f1 = train_lossself.dens1(self.gp1(C1), training=training)\n",
    "        # f2 = self.dens2(self.gp2(C2), training=training)\n",
    "        # f3 = self.dens3(self.gp3(C3), training=training)\n",
    "        # f4 = self.dens4(self.gp4(C4), training=training)\n",
    "        f5 = self.dens5(self.flatten(C5), training=training)\n",
    "        # x = self.add([f1,f2,f3,f4,f5])\n",
    "        x = f5\n",
    "        x = self.dens_int(x, training=training)\n",
    "        x = self.dens_out(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ResNetModelTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "dat = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = dat[0]\n",
    "x_test, y_test = dat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res = np.expand_dims(np.repeat(np.expand_dims(np.pad(x_train[:,0::2, 0::2], [[0,0],[0,1],[0,1]]), axis=-1), 15, axis=-1), axis=-1)\n",
    "x_test_res = np.expand_dims(np.repeat(np.expand_dims(np.pad(x_test[:,0::2, 0::2], [[0,0],[0,1],[0,1]]), axis=-1), 15, axis=-1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_res = np.expand_dims(np.pad(x_train[:,0::2, 0::2], [[0,0],[0,1],[0,1]]), axis=-1)\n",
    "# x_test_res = np.expand_dims(np.pad(x_test[:,0::2, 0::2], [[0,0],[0,1],[0,1]]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3269312381744385, Accuracy: 85.61698913574219, Test Loss: 0.33408501744270325, Test Accuracy: 85.59695434570312\n",
      "Epoch 2, Loss: 0.1531522274017334, Accuracy: 94.39102172851562, Test Loss: 0.460206001996994, Test Accuracy: 83.95433044433594\n",
      "Epoch 3, Loss: 0.0960620790719986, Accuracy: 96.61457824707031, Test Loss: 0.3936728239059448, Test Accuracy: 85.49679565429688\n",
      "train step - 2400/60000        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-99f85bfd3ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mi_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mscene_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mLOG_STEP\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/tamp/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "LOG_STEP = 100\n",
    "\n",
    "N_train = len(x_train_res)\n",
    "N_test = len(x_test_res)\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for _ in range(5000):\n",
    "        scene_data, success, skey = x_train_res[i_step], y_train[i_step], None\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success>=5)\n",
    "        i_step += 1\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            train_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch, dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "\n",
    "    i_step = 0\n",
    "    scene_batch, success_batch = [], []\n",
    "    for _ in range(5000):\n",
    "        scene_data, success, skey = x_test_res[i_step], y_test[i_step], None\n",
    "        scene_batch.append(scene_data)\n",
    "        success_batch.append(success>=5)\n",
    "        i_step += 1\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            test_step(np.array(scene_batch, dtype=np.float32), np.array(success_batch,dtype=np.int))\n",
    "            scene_batch, success_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14230504631996155, Accuracy: 95.68333435058594, Test Loss: 0.05939216911792755, Test Accuracy: 97.91999816894531\n",
      "Epoch 2, Loss: 0.04611682891845703, Accuracy: 98.52166748046875, Test Loss: 0.050787363201379776, Test Accuracy: 98.18999481201172\n",
      "Epoch 3, Loss: 0.024921394884586334, Accuracy: 99.20832824707031, Test Loss: 0.0528109110891819, Test Accuracy: 98.23999786376953\n",
      "Epoch 4, Loss: 0.016222544014453888, Accuracy: 99.46666717529297, Test Loss: 0.05345192924141884, Test Accuracy: 98.5\n",
      "Epoch 5, Loss: 0.010034429840743542, Accuracy: 99.66666412353516, Test Loss: 0.05353892967104912, Test Accuracy: 98.41999816894531\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=uint8, numpy=\n",
       "array([3, 4, 6, 2, 3, 4, 8, 6, 4, 2, 6, 3, 9, 0, 1, 0, 8, 3, 6, 0, 8, 0,\n",
       "       2, 2, 2, 0, 2, 2, 1, 3, 3, 7], dtype=uint8)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
