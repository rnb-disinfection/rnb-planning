{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "PROJ_DIR = os.environ[\"RNB_PLANNING_DIR\"]\n",
    "os.chdir(os.path.join(PROJ_DIR, \"src\"))\n",
    "\n",
    "from pkg.utils.utils_python3 import *\n",
    "DATA_PATH = os.path.join(PROJ_DIR, \"data\")\n",
    "LAT_DATA_PATH = os.path.join(DATA_PATH, \"latticized\")\n",
    "MODEL_PATH = os.path.join(PROJ_DIR, \"model\")\n",
    "LAT_MODEL_PATH = os.path.join(MODEL_PATH,\"latticized_full\")\n",
    "try_mkdir(MODEL_PATH)\n",
    "try_mkdir(LAT_MODEL_PATH)\n",
    "GRASP_FOLDER = \"grasp\"\n",
    "ARM10_FOLDER = \"arm_10\"\n",
    "ARM05_FOLDER = \"arm_05\"\n",
    "FULLS_FOLDER = \"full_scene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT_TYPE_NAME=\"indy7\"\n",
    "ROBOT_DATA_ROOT = os.path.join(LAT_DATA_PATH, ROBOT_TYPE_NAME)\n",
    "# ROBOT_DATA_ROOT = LAT_DATA_PATH\n",
    "ROBOT_MODEL_ROOT =  os.path.join(LAT_MODEL_PATH, ROBOT_TYPE_NAME)\n",
    "ARM_FOLDER = ARM10_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = sorted(os.listdir(ROBOT_DATA_ROOT))\n",
    "DATASET_TRAIN = dataset_list[:10]\n",
    "DATASET_TEST = dataset_list[10:15]\n",
    "print(DATASET_TRAIN)\n",
    "print(DATASET_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_TRAIN = ['20210214-232708', '20210215-041031', '20210215-085110', '20210215-133753', '20210215-184319', \n",
    "#                  '20210216-005455', '20210216-054418', '20210216-104554', '20210216-152114', '20210216-201729']\n",
    "# DATASET_TEST = ['20210217-010926', '20210217-063641', '20210217-113319', '20210217-162106', '20210217-205606']\n",
    "# DATASET_TRAIN = ['20210219-091338', '20210219-124428', '20210219-234147', '20210220-035639', '20210220-080119', \n",
    "#                  '20210220-122304', '20210220-160737', '20210220-194129', '20210220-234400', '20210221-043209']\n",
    "# DATASET_TEST = ['20210221-082144', '20210221-123619', '20210221-160542', '20210221-195509', '20210221-234239']\n",
    "FULL_SHAPE = (60,60,60)\n",
    "\n",
    "dataset_train = []\n",
    "for dataset in DATASET_TRAIN:\n",
    "    file_list = sorted(os.listdir(os.path.join(ROBOT_DATA_ROOT, dataset, FULLS_FOLDER)))\n",
    "    for file in file_list:\n",
    "        dataset_train.append(os.path.join(ROBOT_DATA_ROOT, dataset, FULLS_FOLDER, file))\n",
    "print(\"train set: {}\".format(len(dataset_train)))        \n",
    "\n",
    "\n",
    "dataset_test = []\n",
    "for dataset in DATASET_TEST:\n",
    "    file_list = sorted(os.listdir(os.path.join(ROBOT_DATA_ROOT, dataset, FULLS_FOLDER)))\n",
    "    for file in file_list:\n",
    "        dataset_test.append(os.path.join(ROBOT_DATA_ROOT, dataset, FULLS_FOLDER, file))\n",
    "print(\"train set: {}\".format(len(dataset_test)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    full_data = load_pickle(data_path)\n",
    "    full_tar_idx = full_data[b'tar']\n",
    "    full_obj_idx = full_data[b'obj']\n",
    "    full_tool_idx = full_data[b'tool']\n",
    "    reach_lb = full_data[b'reach']\n",
    "    retrieve_lb = full_data[b'retrieve']\n",
    "    full_tool_img = np.zeros(FULL_SHAPE)\n",
    "    full_tool_img[np.unravel_index(full_tool_idx, shape=FULL_SHAPE)] = 1\n",
    "    full_obj_img = np.zeros(FULL_SHAPE)\n",
    "    full_obj_img[np.unravel_index(full_obj_idx, shape=FULL_SHAPE)] = 1\n",
    "    full_tar_img = np.zeros(FULL_SHAPE)\n",
    "    full_tar_img[np.unravel_index(full_tar_idx, shape=FULL_SHAPE)] = 1\n",
    "    full_img = np.stack([full_tool_img, full_obj_img, full_tar_img], axis=-1)\n",
    "    label = np.array([reach_lb, retrieve_lb])\n",
    "    return full_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.planning.filtering.lattice_model.lattice_model_full import *\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ResNetModelTP_FULL()\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT_MODEL_ROOT =  os.path.join(LAT_MODEL_PATH, ROBOT_TYPE_NAME)\n",
    "current_time = get_now()\n",
    "logpath = os.path.join(ROBOT_MODEL_ROOT, current_time)\n",
    "try_mkdir(logpath)\n",
    "train_log_dir = os.path.join(logpath, 'train')\n",
    "test_log_dir = os.path.join(logpath, 'test')\n",
    "model_log_dir = os.path.join(logpath, 'model_{}/')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "shutil.copy(os.path.join(PROJ_DIR,'src', 'pkg','planning','filtering','lattice_model','lattice_model_full.py' ), logpath)\n",
    "print(f'Log path: {logpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_S = 0\n",
    "EPOCHS_E = 20\n",
    "BATCH_SIZE = 16\n",
    "LOG_STEP = 100\n",
    "N_train = len(dataset_train)\n",
    "N_test = len(dataset_test)\n",
    "\n",
    "for epoch in range(EPOCHS_S, EPOCHS_E):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    random.shuffle(dataset_train)\n",
    "    i_step = 0\n",
    "    data_batch, label_batch = [], []\n",
    "    for data_path in dataset_train:\n",
    "        i_step += 1\n",
    "        full_img, label = load_data(data_path)\n",
    "        data_batch.append(full_img)\n",
    "        label_batch.append(label)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            data_batch = np.array(data_batch)\n",
    "            label_batch = np.array(label_batch, dtype=np.int)\n",
    "            train_step(data_batch, label_batch)\n",
    "            data_batch, label_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"train step - {}/{}        \".format(i_step, N_train), end = '\\r')\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    i_step = 0\n",
    "    data_batch, label_batch = [], []\n",
    "    for data_path in dataset_test:\n",
    "        i_step += 1\n",
    "        full_img, label = load_data(data_path)\n",
    "        data_batch.append(full_img)\n",
    "        label_batch.append(label)\n",
    "        if i_step%BATCH_SIZE==0:\n",
    "            data_batch = np.array(data_batch)\n",
    "            label_batch = np.array(label_batch, dtype=np.int)\n",
    "            test_step(data_batch, label_batch)\n",
    "            data_batch, label_batch = [], []\n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            \n",
    "    model.save(model_log_dir.format(epoch + 1))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=================================================================\")\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )\n",
    "    print(\"=================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDY7\n",
    "=================================================================\n",
    "Epoch 1, Loss: 0.7103465795516968, Accuracy: 50.663936614990234, Test Loss: 0.710077166557312, Test Accuracy: 52.84455490112305\n",
    "=================================================================\n",
    "\n",
    "=================================================================\n",
    "Epoch 4, Loss: 0.3553343713283539, Accuracy: 85.30850219726562, Test Loss: 0.3638063967227936, Test Accuracy: 84.30488586425781\n",
    "=================================================================\n",
    "\n",
    "=================================================================\n",
    "Epoch 9, Loss: 0.21100938320159912, Accuracy: 92.50698852539062, Test Loss: 0.47806230187416077, Test Accuracy: 83.81410217285156\n",
    "=================================================================\n",
    "\n",
    "=================================================================\n",
    "Epoch 10, Loss: 0.17595909535884857, Accuracy: 93.27076721191406, Test Loss: 0.6171865463256836, Test Accuracy: 84.01441955566406\n",
    "=================================================================\n",
    "\n",
    "PANDA\n",
    "=================================================================\n",
    "Epoch 20, Loss: 0.6762306690216064, Accuracy: 59.23544692993164, Test Loss: 0.8472959995269775, Test Accuracy: 58.97563552856445\n",
    "=================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "last_model = sorted(os.listdir(ROBOT_MODEL_ROOT))[-1]\n",
    "logpath = os.path.join(ROBOT_MODEL_ROOT, last_model)\n",
    "\n",
    "model_epoch_list = []\n",
    "acc_epoch_list = []\n",
    "loss_epoch_list = []\n",
    "last_save = sorted([item for item in os.listdir(logpath) if item.startswith(\"model\")], key=lambda x: int(x[6:]))[-1]\n",
    "# last_save = 'model_1'\n",
    "model_log_dir = os.path.join(logpath, last_save)\n",
    "\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(model_log_dir)\n",
    "\n",
    "@tf.function\n",
    "def inference(images):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    return predictions\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "@tf.function\n",
    "def calc_loss(labels, predictions):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    return loss_object(labels, predictions)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LOG_STEP = 100\n",
    "N_test = len(dataset_test)\n",
    "gtimer = GlobalTimer.instance()\n",
    "gtimer.reset()\n",
    "\n",
    "full_img, label = load_data(dataset_test[0])\n",
    "res = inference(np.array([full_img]))\n",
    "\n",
    "i_step = 0\n",
    "res_list = []\n",
    "label_list = []\n",
    "loss_list= []\n",
    "img_batch = []\n",
    "label_batch = [] \n",
    "for data_path in dataset_test:\n",
    "    i_step += 1\n",
    "    full_img, label = load_data(data_path)\n",
    "    img_batch.append(full_img)\n",
    "    label_batch.append(label)\n",
    "    if len(img_batch)==BATCH_SIZE:\n",
    "        img_batch = np.array(img_batch)\n",
    "        with gtimer.block(\"inference\"):\n",
    "            res = inference(img_batch)\n",
    "        loss = calc_loss(label_batch, res)\n",
    "        res_list = res_list + list(res.numpy()>0.5)\n",
    "        label_list = label_list + label_batch\n",
    "        loss_list.append(loss.numpy())\n",
    "        img_batch = []\n",
    "        label_batch = [] \n",
    "    if i_step%LOG_STEP==0:\n",
    "        print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "res_list = np.array(res_list)[:5000,1]\n",
    "label_list = np.array(label_list)[:5000,1]\n",
    "loss_list = np.array(loss_list)[:5000]\n",
    "\n",
    "acc = np.mean(np.equal(res_list, label_list)) * 100\n",
    "mean_loss = np.mean(loss_list)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=================================================================\")\n",
    "print(\n",
    "    f'Test Loss: {mean_loss} \\n'\n",
    "    f'Test Accuracy: {acc} \\n'\n",
    "    f'TP / FN / ACC: {np.sum(np.logical_and(res_list, label_list))}, ' \n",
    "    f'{np.sum(np.logical_and(np.logical_not(res_list), label_list))}, ' \n",
    "    f'{round(np.mean(res_list[np.where(label_list)])*100,2)}\\n'\n",
    "    f'FP / TN / ACC: {np.sum(np.logical_and(res_list, np.logical_not(label_list)))}, '\n",
    "    f'{np.sum(np.logical_and(np.logical_not(res_list), np.logical_not(label_list)))}, '\n",
    "    f'{round(np.mean(np.logical_not(res_list[np.where(np.logical_not(label_list))]))*100,2)}\\n'\n",
    "    f'PACC / NACC / TACC: {round(np.mean(label_list[np.where(res_list)])*100,2)}, '\n",
    "    f'{round(np.mean(np.logical_not(label_list[np.where(np.logical_not(res_list))]))*100,2)}, '\n",
    "    f'{round(np.mean(res_list==label_list)*100,2)}\\n'\n",
    ")\n",
    "print(\"=================================================================\")\n",
    "print(\"\")\n",
    "print(gtimer)\n",
    "model_epoch_list.append(last_save)\n",
    "acc_epoch_list.append(acc)\n",
    "loss_epoch_list.append(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "last_model = sorted(os.listdir(ROBOT_MODEL_ROOT))[-1]\n",
    "logpath = os.path.join(ROBOT_MODEL_ROOT, last_model)\n",
    "\n",
    "model_epoch_list = []\n",
    "acc_epoch_list = []\n",
    "loss_epoch_list = []\n",
    "# last_save = sorted([item for item in os.listdir(logpath) if item.startswith(\"model\")])[-1]\n",
    "# last_save = 'model_1'\n",
    "for last_save in sorted([item for item in os.listdir(logpath) if item.startswith(\"model\")], key=lambda x: int(x[6:])):\n",
    "    model_log_dir = os.path.join(logpath, last_save)\n",
    "\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model(model_log_dir)\n",
    "\n",
    "    @tf.function\n",
    "    def inference(images):\n",
    "        # training=False is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=False)\n",
    "        return predictions\n",
    "\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    @tf.function\n",
    "    def calc_loss(labels, predictions):\n",
    "        # training=False is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        return loss_object(labels, predictions)\n",
    "    \n",
    "    BATCH_SIZE = 50\n",
    "    LOG_STEP = 100\n",
    "    N_test = len(dataset_test)\n",
    "    gtimer = GlobalTimer.instance()\n",
    "    gtimer.reset()\n",
    "\n",
    "    i_step = 0\n",
    "    res_list = []\n",
    "    label_list = []\n",
    "    loss_list= []\n",
    "    img_batch = []\n",
    "    label_batch = [] \n",
    "    for data_path in dataset_test:\n",
    "        i_step += 1\n",
    "        full_img, label = load_data(data_path)\n",
    "        img_batch.append(full_img)\n",
    "        label_batch.append(label)\n",
    "        if len(img_batch)==BATCH_SIZE:\n",
    "            with gtimer.block(\"inference\"):\n",
    "                res = inference(np.array(img_batch))\n",
    "            loss = calc_loss(label_batch, res)\n",
    "            res_list = res_list + list(res.numpy()>0.5)\n",
    "            label_list = label_list + label_batch\n",
    "            loss_list.append(loss.numpy())\n",
    "            img_batch = []\n",
    "            label_batch = [] \n",
    "        if i_step%LOG_STEP==0:\n",
    "            print(\"test step - {}/{}        \".format(i_step, N_test), end = '\\r')\n",
    "\n",
    "    res_list = np.array(res_list)[:5000,1]\n",
    "    label_list = np.array(label_list)[:5000,1]\n",
    "    loss_list = np.array(loss_list)[:5000]\n",
    "    \n",
    "    acc = np.mean(np.equal(res_list, label_list)) * 100\n",
    "    mean_loss = np.mean(loss_list)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=================================================================\")\n",
    "    print(\n",
    "        f'Test Loss: {mean_loss} \\n'\n",
    "        f'Test Accuracy: {acc} \\n'\n",
    "        f'TP / FN / ACC: {np.sum(np.logical_and(res_list, label_list))}, ' \n",
    "        f'{np.sum(np.logical_and(np.logical_not(res_list), label_list))}, ' \n",
    "        f'{round(np.mean(res_list[np.where(label_list)])*100,2)}\\n'\n",
    "        f'FP / TN / ACC: {np.sum(np.logical_and(res_list, np.logical_not(label_list)))}, '\n",
    "        f'{np.sum(np.logical_and(np.logical_not(res_list), np.logical_not(label_list)))}, '\n",
    "        f'{round(np.mean(np.logical_not(res_list[np.where(np.logical_not(label_list))]))*100,2)}\\n'\n",
    "        f'PACC / NACC / TACC: {round(np.mean(label_list[np.where(res_list)])*100,2)}, '\n",
    "        f'{round(np.mean(np.logical_not(label_list[np.where(np.logical_not(res_list))]))*100,2)}, '\n",
    "        f'{round(np.mean(res_list==label_list)*100,2)}\\n'\n",
    "    )\n",
    "    print(\"=================================================================\")\n",
    "    print(\"\")\n",
    "    print(gtimer)\n",
    "    model_epoch_list.append(last_save)\n",
    "    acc_epoch_list.append(acc)\n",
    "    loss_epoch_list.append(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(model_epoch_list)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(acc_epoch_list)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(loss_epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(ROBOT_TYPE_NAME+\".json\", {\"epoch\": np.array(model_epoch_list), \"acc\": np.array(acc_epoch_list), \"loss\": np.array(loss_epoch_list)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = dataset_train[0]\n",
    "# full_data = load_pickle(data_path)\n",
    "# full_tar_idx = full_data[b'tar']\n",
    "# full_tool_idx = full_data[b'tool']\n",
    "# save_json(\"tool.json\", np.array(np.unravel_index(full_tool_idx, shape=FULL_SHAPE)).transpose())\n",
    "# save_json(\"tar.json\", np.array(np.unravel_index(full_tar_idx, shape=FULL_SHAPE)).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
