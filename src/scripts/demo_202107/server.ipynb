{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import cv2\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "from _thread import *\n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_color = Queue()\n",
    "queue_depth = Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client가 접속하면 color, depth image 정보를 string으로 encoding해서 client로 송신\n",
    "def threaded(socket, addr, queue_color, queue_depth): \n",
    "\n",
    "    print('Connected by :', addr[0], ':', addr[1]) \n",
    "\n",
    "    while True: \n",
    "\n",
    "        try:\n",
    "            data = socket.recv(1024)\n",
    "\n",
    "            if not data: \n",
    "                print('Disconnected by ' + addr[0],':',addr[1])\n",
    "                break\n",
    "\n",
    "            stringData_color = queue_color.get()\n",
    "            stringData_depth = queue_depth.get()\n",
    "            stringData = stringData_color + stringData_depth\n",
    "            socket.send(str(len(stringData_color)).ljust(16).encode())\n",
    "            socket.send(str(len(stringData_depth)).ljust(16).encode())\n",
    "            socket.send(stringData)\n",
    "\n",
    "        except ConnectionResetError as e:\n",
    "\n",
    "            print('Disconnected by ' + addr[0],':',addr[1])\n",
    "            break\n",
    "             \n",
    "    socket.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_streaming(queue_color, queue_depth):\n",
    "    \n",
    "    ## 0~100에서 95의 이미지 품질로 설정 (default = 95)\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]\n",
    "    \n",
    "    # Configure depth and color streams\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()   \n",
    "\n",
    "    # Set stream resolution\n",
    "    #config.enable_stream(rs.stream.depth, DEPTHMAP_SIZE[1], DEPTHMAP_SIZE[0], rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    #config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)\n",
    "    #config.enable_stream(rs.stream.color, IMAGE_SIZE[1], IMAGE_SIZE[0], rs.format.bgr8, 30)\n",
    "    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "    #config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)\n",
    "\n",
    "    # Start streaming\n",
    "    profile = pipeline.start(config)\n",
    "    depth_sensor = profile.get_device().first_depth_sensor()\n",
    "    depth_sensor.set_option(rs.option.visual_preset, 3)\n",
    "    # Custom = 0, Default = 1, Hand = 2, HighAccuracy = 3, HighDensity = 4, MediumDensity = 5\n",
    "    depth_scale = depth_sensor.get_depth_scale()\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            aligned_frames = align.process(frames)\n",
    "\n",
    "            #aligned_color_frame = aligned_frames.get_color_frame()\n",
    "            #depth_frame = aligned_frames.get_depth_frame()\n",
    "\n",
    "            aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "            #depth_frame = frames.get_depth_frame()\n",
    "            #color_frame = frames.get_color_frame()\n",
    "            if not aligned_depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            #depth_to_color_extrins = depth_frame.profile.get_extrinsics_to(color_frame.profile)\n",
    "            depth_intrins = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "            #depth_intrins = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "            #color_intrins = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "            #intrins = depth_intrins\n",
    "\n",
    "            # Convert images to numpy arrays\n",
    "            depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "\n",
    "            # 뭐 작업을 시작하라는 신호가 오면 client에서 쏘라고 하고, 그 뒤에 읽어들여야?\n",
    "            #data = connection.recv(1024)\n",
    "\n",
    "            # cv2. imencode(ext, img [, params])\n",
    "            # encode_param의 형식으로 frame을 jpg로 이미지를 인코딩한다.\n",
    "            result, color_img = cv2.imencode('.jpg', color_image, encode_param)\n",
    "            result, depth_img = cv2.imencode('.png', depth_image, encode_param)\n",
    "\n",
    "            # frame을 String 형태로 변환\n",
    "            color_data = np.array(color_img)\n",
    "            depth_data = np.array(depth_img)\n",
    "            stringData_color = color_data.tostring()\n",
    "            stringData_depth = depth_data.tostring()\n",
    "\n",
    "            # color image와 depth image를 queue에 저장\n",
    "            queue_color.put(stringData_color)\n",
    "            queue_depth.put(stringData_depth)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    finally:\n",
    "        # Stop streaming\n",
    "        pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket created\n",
      "Socket bind complete\n",
      "Socket now listening\n",
      "wait\n",
      "start thread\n",
      "wait\n",
      "('Connected by :', '192.168.0.4', ':', 43412)\n",
      "('Disconnected by 192.168.0.4', ':', 43412)start thread\n",
      "wait\n",
      "\n",
      "('Connected by :', '192.168.0.4', ':', 43418)\n",
      "('Disconnected by 192.168.0.4', ':', 43418)start thread\n",
      "wait\n",
      " \n",
      "('Connected by :', '192.168.0.4', ':', 43422)\n",
      "('Disconnected by 192.168.0.4', ':', 43422)start thread\n",
      "wait\n",
      "\n",
      "('Connected by :', '192.168.0.4', ':', 43424)\n",
      "('Disconnected by 192.168.0.4', ':', 43424)\n",
      "start thread\n",
      "wait('Connected by :', '192.168.0.4', ':', 43428)\n",
      "\n",
      "('Disconnected by 192.168.0.4', ':', 43428)start thread\n",
      "\n",
      "wait('Connected by :', '192.168.0.4', ':', 43430)\n",
      "\n",
      "('Disconnected by 192.168.0.4', ':', 43430)\n",
      "start thread\n",
      "wait\n",
      "('Connected by :', '192.168.0.4', ':', 43432)\n",
      "('Disconnected by 192.168.0.4', ':', 43432)\n",
      "start thread\n",
      "wait('Connected by :', '192.168.0.4', ':', 43434)\n",
      "\n",
      "('Disconnected by 192.168.0.4', ':', 43434)\n",
      "Frame didn't arrive within 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8f8248324999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wait'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m#connection2, addr2 = server_socket.accept()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36maccept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_socketobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0maccept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_realsocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HOST='192.168.0.4'\n",
    "PORT=8580\n",
    " \n",
    "#TCP 사용\n",
    "server_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "print('Socket created')\n",
    " \n",
    "#서버의 아이피와 포트번호 지정\n",
    "server_socket.bind((HOST,PORT))\n",
    "print('Socket bind complete')\n",
    "\n",
    "# 클라이언트의 접속을 기다린다. (클라이언트 연결을 10개까지 받는다)\n",
    "server_socket.listen(10)\n",
    "print('Socket now listening')\n",
    " \n",
    "# #연결, conn에는 소켓 객체, addr은 소켓에 바인드 된 주소\n",
    "# connection, address = server_socket.accept()\n",
    "# print('Server start')\n",
    "\n",
    "\n",
    "start_new_thread(camera_streaming, (queue_color, queue_depth,))\n",
    "\n",
    "\n",
    "while True: \n",
    "\n",
    "    print('wait')\n",
    "\n",
    "    connection, addr = server_socket.accept()\n",
    "    #connection2, addr2 = server_socket.accept()\n",
    "    \n",
    "    print('start thread')\n",
    "    start_new_thread(threaded, (connection, addr, queue_color, queue_depth,))\n",
    "    #start_new_thread(thread1, (connection1, addr1, queue_color,))\n",
    "    #start_new_thread(thread2, (connection2, addr2, queue_depth,)) \n",
    "\n",
    "server_socket.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket created\n",
      "Socket bind complete\n",
      "Socket now listening\n",
      "Socket connected\n",
      "Start camera streaming\n",
      "Send data to client\n",
      "[Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "#socket에서 수신한 버퍼를 반환하는 함수\n",
    "def recvall(sock, count):\n",
    "    # 바이트 문자열\n",
    "    buf = b''\n",
    "    while count:\n",
    "        newbuf = sock.recv(count)\n",
    "        if not newbuf: return None\n",
    "        buf += newbuf\n",
    "        count -= len(newbuf)\n",
    "    return buf\n",
    " \n",
    "HOST='192.168.0.4'\n",
    "PORT=8580\n",
    " \n",
    "#TCP 사용\n",
    "server_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "print('Socket created')\n",
    " \n",
    "#서버의 아이피와 포트번호 지정\n",
    "server_socket.bind((HOST,PORT))\n",
    "print('Socket bind complete')\n",
    "\n",
    "# 클라이언트의 접속을 기다린다. (클라이언트 연결을 10개까지 받는다)\n",
    "server_socket.listen(10)\n",
    "print('Socket now listening')\n",
    " \n",
    "#연결, conn에는 소켓 객체, addr은 소켓에 바인드 된 주소\n",
    "connection, address = server_socket.accept()\n",
    "print('Socket connected')\n",
    "\n",
    "## 0~100에서 95의 이미지 품질로 설정 (default = 95)\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]\n",
    "\n",
    "print('Start camera streaming')\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()   \n",
    "\n",
    "# Set stream resolution\n",
    "#config.enable_stream(rs.stream.depth, DEPTHMAP_SIZE[1], DEPTHMAP_SIZE[0], rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.color, IMAGE_SIZE[1], IMAGE_SIZE[0], rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "#config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_sensor.set_option(rs.option.visual_preset, 3)\n",
    "# Custom = 0, Default = 1, Hand = 2, HighAccuracy = 3, HighDensity = 4, MediumDensity = 5\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        #aligned_color_frame = aligned_frames.get_color_frame()\n",
    "        #depth_frame = aligned_frames.get_depth_frame()\n",
    "\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        #depth_frame = frames.get_depth_frame()\n",
    "        #color_frame = frames.get_color_frame()\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        #depth_to_color_extrins = depth_frame.profile.get_extrinsics_to(color_frame.profile)\n",
    "        depth_intrins = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #depth_intrins = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #color_intrins = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #intrins = depth_intrins\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        \n",
    "        # 뭐 작업을 시작하라는 신호가 오면 client에서 쏘라고 하고, 그 뒤에 읽어들여야?\n",
    "        #data = connection.recv(1024)\n",
    "        \n",
    "        print('Send data to client')\n",
    "        if (True):\n",
    "            # cv2. imencode(ext, img [, params])\n",
    "            # encode_param의 형식으로 frame을 jpg로 이미지를 인코딩한다.\n",
    "            result, color_img = cv2.imencode('.jpg', color_image, encode_param)\n",
    "            result, depth_img = cv2.imencode('.png', depth_image, encode_param)\n",
    "\n",
    "            # frame을 String 형태로 변환\n",
    "            color_data = np.array(color_img)\n",
    "            depth_data = np.array(depth_img)\n",
    "            stringData_color = color_data.tostring()\n",
    "            stringData_depth = depth_data.tostring()\n",
    "            \n",
    "\n",
    "            #서버에 데이터 전송\n",
    "            #(str(len(stringData))).encode().ljust(16)\n",
    "            server_socket.sendall((str(len(stringData_color))).encode().ljust(16) + stringData_color)\n",
    "            print(str(len(stringData_color)))\n",
    "            print(str(len(stringData_depth)))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     # client에서 받은 stringData의 크기 (==(str(len(stringData))).encode().ljust(16))\n",
    "    \n",
    "#     length = recvall(connection, 16)\n",
    "#     stringData1 = recvall(connection, int(length1))\n",
    "#     data1 = np.fromstring(stringData1, dtype = 'uint8')\n",
    "    \n",
    "#     #data를 디코딩한다.\n",
    "#     frame1 = cv2.imdecode(data1, cv2.IMREAD_COLOR)\n",
    "#     cv2.imshow('ImageWindow',frame1)\n",
    "#     key = cv2.waitKey(1)\n",
    "    \n",
    "#     if (key == 115):\n",
    "#         # If 's' pressed, save color, depth images (far from table)\n",
    "#         cv2.imwrite('/home/jhkim/Projects/rnb-planning/src/scripts/demo_202107/test_tcp/color.jpg', frame1) \n",
    "#         #cv2.imwrite('/home/jhkim/Projects/rnb-planning/src/scripts/demo_202107/test_tcp/depth.png', frame2)    \n",
    "    \n",
    "#     if (key == 27):\n",
    "#         # If 'esc' pressed, stop streaming and exit\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'client_socket' is not defined\n"
     ]
    }
   ],
   "source": [
    "## 0~100에서 95의 이미지 품질로 설정 (default = 95)\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()   \n",
    "\n",
    "# Set stream resolution\n",
    "#config.enable_stream(rs.stream.depth, DEPTHMAP_SIZE[1], DEPTHMAP_SIZE[0], rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.color, IMAGE_SIZE[1], IMAGE_SIZE[0], rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "#config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_sensor.set_option(rs.option.visual_preset, 3)\n",
    "# Custom = 0, Default = 1, Hand = 2, HighAccuracy = 3, HighDensity = 4, MediumDensity = 5\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        #aligned_color_frame = aligned_frames.get_color_frame()\n",
    "        #depth_frame = aligned_frames.get_depth_frame()\n",
    "\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        #depth_frame = frames.get_depth_frame()\n",
    "        #color_frame = frames.get_color_frame()\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        #depth_to_color_extrins = depth_frame.profile.get_extrinsics_to(color_frame.profile)\n",
    "        depth_intrins = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #depth_intrins = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #color_intrins = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "        #intrins = depth_intrins\n",
    "        #print(depth_intrins)\n",
    "        #print(color_intrins)\n",
    "        #print(depth_to_color_extrins)\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        #print(depth_image.shape)\n",
    "        #print(color_image.shape)\n",
    "        #fusion_image = np.concatenate((color_image, depth_image), axis=1)\n",
    "        \n",
    "        # cv2. imencode(ext, img [, params])\n",
    "        # encode_param의 형식으로 frame을 jpg로 이미지를 인코딩한다.\n",
    "        #result1, color_img = cv2.imencode('.jpg', color_image, encode_param)\n",
    "        #result2, depth_img = cv2.imencode('.png', depth_image, encode_param)\n",
    "        result, img = cv2.imencode('.jpg', color_image, encode_param)\n",
    "        \n",
    "        # frame을 String 형태로 변환\n",
    "        #data1 = np.array(color_img)\n",
    "        #data2 = np.array(depth_img)\n",
    "        data = np.array(img)\n",
    "        #stringData1 = data1.tostring()\n",
    "        #stringData2 = data2.tostring()\n",
    "        stringData = data.tostring()\n",
    "\n",
    "        #서버에 데이터 전송\n",
    "        #(str(len(stringData))).encode().ljust(16)\n",
    "        client_socket.sendall((str(len(stringData))).encode().ljust(16) + stringData)\n",
    "        #client_socket.sendall((str(len(stringData2))).encode().ljust(16) + stringData2)\n",
    "        \n",
    "#         if (key == 27):\n",
    "#             # If 'esc' pressed, stop streaming and exit\n",
    "#             cv2.destroyAllWindows()\n",
    "#             break\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
