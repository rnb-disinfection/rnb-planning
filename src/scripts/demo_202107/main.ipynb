{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.join(os.environ[\"RNB_PLANNING_DIR\"], 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.global_config import RNB_PLANNING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory setting\n",
    "DEMO_DIR = os.path.join(RNB_PLANNING_DIR, \"src/scripts/demo_202107\")\n",
    "CONFIG_DIR = os.path.join(DEMO_DIR, \"configs\")\n",
    "SAVE_DIR = os.path.join(DEMO_DIR, \"save_img\")\n",
    "CROP_DIR = os.path.join(DEMO_DIR, \"crop_img\")\n",
    "MODEL_DIR = os.path.join(DEMO_DIR, \"model_CAD\")\n",
    "\n",
    "# Table dimension\n",
    "T_Width = 1.8\n",
    "T_Height = 0.785\n",
    "T_Depth = 0.734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection command:\n",
      "indy0: False\n"
     ]
    }
   ],
   "source": [
    "from pkg.controller.combined_robot import *\n",
    "from pkg.project_config import *\n",
    "\n",
    "crob = CombinedRobot(robots_on_scene=[\n",
    "    RobotConfig(0, RobotType.indy7, None,\n",
    "                INDY_IP)]\n",
    "              , connection_list=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.geometry.builder.scene_builder import SceneBuilder\n",
    "s_builder = SceneBuilder(None)\n",
    "# s_builder.reset_reference_coord(ref_name=\"floor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please create a subscriber to the marker\n",
      "publication OK\n",
      "published: [0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# xyz_rpy_robots = s_builder.detect_items(level_mask=[DetectionLevel.ROBOT])\n",
    "xyz_rpy_robots = {\"indy0\": ((0,0,0), (0,0,np.pi))}\n",
    "crob.update_robot_pos_dict(xyz_rpy_robots=xyz_rpy_robots)\n",
    "gscene = s_builder.create_gscene(crob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please create a subscriber to the marker\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pkg.geometry.geometry.GeometryItem at 0x7fd915b171d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pkg.geometry.geotype import GEOTYPE\n",
    "gscene.create_safe(gtype=GEOTYPE.CYLINDER, name=\"cam\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.061,0.061,0.026), center=(-0.0785,0,0.013), rpy=(0,0,0), \n",
    "                   color=(0.8,0.8,0.8,0.5), display=True, fixed=True, collision=False)\n",
    "\n",
    "gscene.create_safe(gtype=GEOTYPE.CYLINDER, name=\"cam_col\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.081,0.081,0.046), center=(-0.0785,0,0.013), rpy=(0,0,0), \n",
    "                   color=(0.0,1,0,0.3), display=True, fixed=True, collision=True)\n",
    "\n",
    "viewpoint = gscene.create_safe(gtype=GEOTYPE.SPHERE, name=\"viewpoint\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.01,0.01,0.01), center=(0,0,0), rpy=(0,0,0), \n",
    "                   color=(1,0,0,0.3), display=True, fixed=True, collision=False, parent=\"cam\")\n",
    "\n",
    "gscene.create_safe(gtype=GEOTYPE.CYLINDER, name=\"body\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.067,0.067,0.0335), center=(-0.0785,0,-0.01675), rpy=(0,0,0), \n",
    "                   color=(0.8,0.8,0.8,1), display=True, fixed=True, collision=False)\n",
    "\n",
    "gscene.create_safe(gtype=GEOTYPE.CYLINDER, name=\"body_col\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.087,0.087,0.0535), center=(-0.0785,0,-0.01675), rpy=(0,0,0), \n",
    "                   color=(0.0,1,0,0.3), display=True, fixed=True, collision=True)\n",
    "\n",
    "gscene.create_safe(gtype=GEOTYPE.SPHERE, name=\"backhead\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.067,0.067,0.067), center=(-0.0785,0,-0.0335), rpy=(0,0,0), \n",
    "                   color=(0.8,0.8,0.8,1), display=True, fixed=True, collision=False)\n",
    "\n",
    "gscene.create_safe(gtype=GEOTYPE.SPHERE, name=\"backhead_col\", link_name=\"indy0_tcp\", \n",
    "                   dims=(0.087,0.087,0.087), center=(-0.0785,0,-0.0335), rpy=(0,0,0), \n",
    "                   color=(0.0,1,0,0.3), display=True, fixed=True, collision=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = [0]*6\n",
    "# viewpoint.draw_traj_coords([Q])\n",
    "# gscene.show_pose(crob.home_pose)\n",
    "# inspect_arguments(gscene.create_safe)\n",
    "# gscene.add_highlight_axis(\"ax\", \"tcp\", \"indy0_tcp\", center=(0,)*3, orientation_mat=np.identity(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.206 -0.198 -0.958 -0.572]\n",
      " [-0.149 -0.974  0.169  0.352]\n",
      " [-0.967  0.108 -0.23   0.981]\n",
      " [ 0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "# Set joint value\n",
    "Q = [-pi/10,-pi/10, -pi/7, pi/20, -pi/3, 0]\n",
    "crob.joint_move_make_sure(Q)\n",
    "gscene.show_pose(Q)\n",
    "\n",
    "# Add coordinate of viewpoint geometry(camera)\n",
    "viewpoint.draw_traj_coords([Q])\n",
    "\n",
    "# Transformation from base link to camera\n",
    "T_v = viewpoint.get_tf(list2dict(Q, gscene.joint_names), \"base_link\")\n",
    "print(np.round(T_v, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def camera_streaming():\n",
    "    # Configure depth and color streams\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()   \n",
    "\n",
    "    # Set stream resolution\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    #config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "    #config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)\n",
    "\n",
    "    # Start streaming\n",
    "    profile = pipeline.start(config)\n",
    "    depth_sensor = profile.get_device().first_depth_sensor()\n",
    "    depth_sensor.set_option(rs.option.visual_preset, 3)\n",
    "    # Custom = 0, Default = 1, Hand = 2, HighAccuracy = 3, HighDensity = 4, MediumDensity = 5\n",
    "    depth_scale = depth_sensor.get_depth_scale()\n",
    "    align_to = rs.stream.depth\n",
    "    align = rs.align(align_to)\n",
    "\n",
    "    intrins = []\n",
    "    try:\n",
    "        while True:\n",
    "\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            aligned_frames = align.process(frames)\n",
    "\n",
    "            aligned_color_frame = aligned_frames.get_color_frame()\n",
    "            depth_frame = aligned_frames.get_depth_frame()\n",
    "\n",
    "            #aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "            #color_frame = aligned_frames.get_color_frame()\n",
    "            #depth_frame = frames.get_depth_frame()\n",
    "            #color_frame = frames.get_color_frame()\n",
    "            if not depth_frame or not aligned_color_frame:\n",
    "                continue\n",
    "\n",
    "            #depth_intrins = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "            depth_intrins = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "            color_intrins = aligned_color_frame.profile.as_video_stream_profile().intrinsics\n",
    "            intrins = depth_intrins\n",
    "            #print(depth_intrins)\n",
    "            #print(color_intrins)\n",
    "\n",
    "            # Convert images to numpy arrays\n",
    "            depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            color_image = np.asanyarray(aligned_color_frame.get_data())\n",
    "\n",
    "            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "            depth_colormap_dim = depth_colormap.shape\n",
    "            color_colormap_dim = color_image.shape\n",
    "\n",
    "            # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "            if depth_colormap_dim != color_colormap_dim:\n",
    "                resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "                images = np.hstack((resized_color_image, depth_colormap))\n",
    "            else:\n",
    "                images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "            \n",
    "            # Show images\n",
    "            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.imshow('RealSense', images)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if (key == 115):\n",
    "                # If 's' pressed, save color, depth images (far from table)\n",
    "                cv2.imwrite(SAVE_DIR +'/color.jpg', color_image)\n",
    "                cv2.imwrite(SAVE_DIR +'/depth.png', depth_image)\n",
    "            \n",
    "            if (key == 99):\n",
    "                # If 'c' pressed, save color, depth images to refine (near from table)\n",
    "                cv2.imwrite(SAVE_DIR + '/table.jpg', color_image)\n",
    "                cv2.imwrite(SAVE_DIR + '/table.png', depth_image)\n",
    "\n",
    "            if (key == 27):\n",
    "                # If 'esc' pressed, stop streaming and exit\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "\n",
    "        # Stop streaming\n",
    "        pipeline.stop()\n",
    "        return intrins, depth_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming & Save color, depth image\n",
    "# Return is intrinsic parameter, depth_scale of camera\n",
    "cam_intrins, d_scale = camera_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SharedArray as sa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_URI = \"shm://color_img\"\n",
    "MASK_URI = \"shm://mask_img\"\n",
    "REQ_URI = \"shm://request\"\n",
    "RESP_URI = \"shm://response\"\n",
    "\n",
    "color_img_p = sa.attach(IMG_URI)\n",
    "return_img_p = sa.attach(MASK_URI)\n",
    "request_p = sa.attach(REQ_URI)\n",
    "resp_p = sa.attach(RESP_URI)\n",
    "\n",
    "def detect_from_server(image):\n",
    "    color_img_p[:] = image[:]\n",
    "    request_p[:] = 1\n",
    "    while not resp_p[0]:\n",
    "        time.sleep(0.01)\n",
    "    resp_p[:] = 0\n",
    "    return np.copy(return_img_p.astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set color, depth image path\n",
    "color_img_path = SAVE_DIR + '/color_.jpg'\n",
    "depth_img_path = SAVE_DIR + '/depth_.png'\n",
    "\n",
    "# Read color, depth image file, keep 16bit information\n",
    "color_img_read = cv2.imread(color_img_path, flags=cv2.IMREAD_UNCHANGED)\n",
    "depth_img_read = cv2.imread(depth_img_path, flags=cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (720,1280,3) into shape (480,640,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4dcbb037ec38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Output of inference(mask for detected table)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmask_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_from_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_img_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8a1ca8d9195a>\u001b[0m in \u001b[0;36mdetect_from_server\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_from_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcolor_img_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mrequest_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (720,1280,3) into shape (480,640,3)"
     ]
    }
   ],
   "source": [
    "# Output of inference(mask for detected table)\n",
    "mask_out = detect_from_server(color_img_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ca7b190f8231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Crop masking part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask_out\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcolor_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_img_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_img_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvis_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdepth_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_img_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_img_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvis_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#cv2.imwrite(CROP_DIR + '/color_crop.jpg', color_instance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_out' is not defined"
     ]
    }
   ],
   "source": [
    "# Crop masking part\n",
    "vis_mask = (mask_out * 255).astype('uint8')\n",
    "color_instance = cv2.bitwise_and(color_img_read, color_img_read, mask=vis_mask).astype(np.uint16)\n",
    "depth_instance = cv2.bitwise_and(depth_img_read, depth_img_read, mask=vis_mask).astype(np.uint16)\n",
    "#cv2.imwrite(CROP_DIR + '/color_crop.jpg', color_instance)\n",
    "#cv2.imwrite(CROP_DIR + '/depth_crop.png', depth_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "#import subprocess\n",
    "#subprocess.call(['python3', 'detection.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    # Load CAD model of table leg\n",
    "    model_mesh = o3d.io.read_triangle_mesh(MODEL_DIR + '/table_leg_scaling.STL')\n",
    "    #model_pcd = model_mesh.sample_points_uniformly(number_of_points=300)\n",
    "\n",
    "    # Load Depth image to make point clouds\n",
    "    depth = o3d.io.read_image(CROP_DIR + '/depth_crop.png')\n",
    "    depth_pcd = o3d.geometry.PointCloud.create_from_depth_image(depth,\n",
    "                                            o3d.camera.PinholeCameraIntrinsic(cam_intrins.width,\n",
    "                                            cam_intrins.height, cam_intrins.fx, cam_intrins.fy,\n",
    "                                            cam_intrins.ppx, cam_intrins.ppy),\n",
    "                                            depth_scale = 1/d_scale)\n",
    "    #depth_pcd = o3d.geometry.PointCloud.create_from_depth_image(depth, o3d.camera.PinholeCameraIntrinsic(640, 480, \n",
    "    #                                                                                                  461.734375, 462.06640625, \n",
    "    #                                                                                                  350.4140625, 244.541015625), depth_scale = 4000.0)\n",
    "\n",
    "    # Remove noise points which put very far from camera\n",
    "    thres = np.linalg.norm(depth_pcd.get_center())\n",
    "    depth_pcd = o3d.geometry.PointCloud.create_from_depth_image(depth,\n",
    "                                            o3d.camera.PinholeCameraIntrinsic(cam_intrins.width,\n",
    "                                            cam_intrins.height, cam_intrins.fx, cam_intrins.fy,\n",
    "                                            cam_intrins.ppx, cam_intrins.ppy),\n",
    "                                            depth_scale = 1/d_scale, depth_trunc = thres * 1.55)\n",
    "    #depth_pcd = o3d.geometry.PointCloud.create_from_depth_image(depth, o3d.camera.PinholeCameraIntrinsic(640, 480, \n",
    "    #                                                                                                  461.734375, 462.06640625, \n",
    "    #                                                                                                  350.4140625, 244.541015625), depth_scale = 4000.0, depth_trunc = thres * 1.5)\n",
    "\n",
    "    #o3d.visualization.draw_geometries([depth_pcd])\n",
    "\n",
    "    # Convert point clouds to numpy array\n",
    "    xyz_points = np.array(depth_pcd.points)\n",
    "\n",
    "    # Kmeans Clustering to classify front, back legs of table\n",
    "    # Ideally, if noise does not exist, then number of cluster is 2\n",
    "    kmeans = KMeans(n_clusters = 2, random_state = 0)\n",
    "    kmeans.fit(xyz_points)\n",
    "\n",
    "    # Re-convert numpy array to point clouds in type of o3d point clouds\n",
    "    pcd1 = o3d.geometry.PointCloud()\n",
    "    pcd2 = o3d.geometry.PointCloud()\n",
    "    #pcd3 = o3d.geometry.PointCloud()\n",
    "    xyz_1 = np.vstack([xyz_points[kmeans.labels_ == 0,0], xyz_points[kmeans.labels_ == 0,1], xyz_points[kmeans.labels_ == 0,2]])\n",
    "    xyz_2 = np.vstack([xyz_points[kmeans.labels_ == 1,0], xyz_points[kmeans.labels_ == 1,1], xyz_points[kmeans.labels_ == 1,2]])\n",
    "    #xyz_3 = np.vstack([xyz_points[kmeans.labels_ == 2,0], xyz_points[kmeans.labels_ == 2,1], xyz_points[kmeans.labels_ == 2,2]])\n",
    "    pcd1.points = o3d.utility.Vector3dVector(xyz_1.T)\n",
    "    pcd2.points = o3d.utility.Vector3dVector(xyz_2.T)\n",
    "    #pcd3.points = o3d.utility.Vector3dVector(xyz_3.T)\n",
    "    FOR_origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    #o3d.visualization.draw_geometries([pcd1, FOR_origin])\n",
    "    return model_mesh, pcd1, pcd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result_original_color(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.transform(transformation)\n",
    "    FOR_origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    \n",
    "    FOR_model = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    FOR_model.transform(transformation)\n",
    "    FOR_model.translate(source_temp.get_center()-FOR_model.get_center())\n",
    "    \n",
    "    FOR_target = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=target.get_center())\n",
    "    o3d.visualization.draw_geometries([source_temp, target, FOR_origin, FOR_model, FOR_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mesh, pcd1, pcd2 = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ICP(model_mesh, pcd):\n",
    "    # Compute ICP to align model(source) to obtained point clouds(target)\n",
    "    target = copy.deepcopy(pcd)\n",
    "    model_pcd = model_mesh.sample_points_uniformly(number_of_points=int(len(np.array(target.points)*0.8)))\n",
    "    source = copy.deepcopy(model_pcd)\n",
    "    #source.translate((-T_Height/2, -T_Depth/2, 0.0), relative=True)\n",
    "    source_cpy = copy.deepcopy(model_pcd)\n",
    "    source_cpy.translate((-T_Height, -T_Depth, 0.0), relative=True)\n",
    "\n",
    "    # Guess Initial Transformation\n",
    "    center = target.get_center()\n",
    "    trans_init = np.identity(4)\n",
    "    trans_init[0:3,3] = center.T\n",
    "    source_cpy.transform(trans_init)\n",
    "    trans_init[0:3,3] = source_cpy.get_center().T\n",
    "    draw_registration_result_original_color(source, target, trans_init)\n",
    "\n",
    "    print(\"Apply point-to-point ICP\")\n",
    "    threshold = 0.12\n",
    "    reg_p2p = o3d.registration.registration_icp(source, target, threshold, trans_init,\n",
    "            o3d.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.registration.ICPConvergenceCriteria(max_iteration = 600000))\n",
    "    print(reg_p2p)\n",
    "    print(\"Transformation is:\")\n",
    "    print(reg_p2p.transformation)\n",
    "    draw_registration_result_original_color(source, target, reg_p2p.transformation)\n",
    "    ICP_result = reg_p2p.transformation    \n",
    "    return ICP_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=9.184710e-03, and correspondence_set size of 4991\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 0.99779692 -0.04054529  0.05251089 -1.55800093]\n",
      " [ 0.05658045  0.93336264 -0.35444723 -0.3632592 ]\n",
      " [-0.03464054  0.35663744  0.93360043  3.06640263]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "Apply point-to-point ICP\n",
      "registration::RegistrationResult with fitness=6.674312e-01, inlier_rmse=3.974112e-02, and correspondence_set size of 291\n",
      "Access transformation to get result.\n",
      "Transformation is:\n",
      "[[ 9.92312828e-01 -1.14006728e-01  4.81426740e-02 -1.42192060e+00]\n",
      " [ 1.23725872e-01  9.22346446e-01 -3.66017681e-01 -9.15843968e-01]\n",
      " [-2.67574606e-03  3.69160535e-01  9.29361792e-01  4.68087679e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geometry::TriangleMesh with 1134 points and 2240 triangles."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICP_result1 = compute_ICP(model_mesh, pcd1)\n",
    "ICP_result2 = compute_ICP(model_mesh, pcd2)\n",
    "\n",
    "FOR_1 = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "#FOR_1.translate((T_Height/2, T_Depth/2, 0.0), relative=True)\n",
    "FOR_1.transform(ICP_result1)\n",
    "#FOR_1.translate(source1.get_center()-FOR_1.get_center(), relative=True)\n",
    "FOR_1.translate((T_Height/2, T_Depth/2, 0.0), relative=True)\n",
    "#source1.transform(reg_p2p.transformation)\n",
    "\n",
    "FOR_2 = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "#FOR_2.translate((T_Height/2, T_Depth/2, 0), relative=True)\n",
    "FOR_2.transform(ICP_result2)\n",
    "#FOR_2.translate(source2.get_center()-FOR_2.get_center(), relative=True)\n",
    "FOR_2.translate((T_Height/2, T_Depth/2, 0), relative=True)\n",
    "#source2.transform(reg_p2p.transformation)center())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_plane_fitting(img_path):\n",
    "    # Find plane through ransac plane fitting\n",
    "    #depth_raw = o3d.io.read_image(\"/home/jhkim/Projects/rnb-planning/src/scripts/demo_202107/save_img/depth.png\")\n",
    "    depth_raw = o3d.io.read_image(img_path)\n",
    "    depth_pcd_raw = o3d.geometry.PointCloud.create_from_depth_image(depth_raw,\n",
    "                                            o3d.camera.PinholeCameraIntrinsic(cam_intrins.width,\n",
    "                                            cam_intrins.height, cam_intrins.fx, cam_intrins.fy,\n",
    "                                            cam_intrins.ppx, cam_intrins.ppy), depth_scale = 1/d_scale)\n",
    "    #depth_pcd_raw = o3d.geometry.PointCloud.create_from_depth_image(depth_raw, o3d.camera.PinholeCameraIntrinsic(640, 480, \n",
    "    #                                                                                                  461.734375, 462.06640625, \n",
    "    #                                                                                                  350.4140625, 244.541015625), depth_scale = 4000.0)\n",
    "\n",
    "    plane_model, inliers = depth_pcd_raw.segment_plane(distance_threshold=0.01,\n",
    "                                             ransac_n=8,\n",
    "                                             num_iterations=2000)\n",
    "    [a, b, c, d] = plane_model\n",
    "    print(\"Coeffs of eq of fitting plane are :\")\n",
    "    print(a,b,c,d)\n",
    "    return plane_model\n",
    "    #print(f\"Plane equation: {a:.5f}x + {b:.5f}y + {c:.5f}z + {d:.5f} = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeffs of eq of fitting plane are :\n",
      "(-0.03278097312858208, 0.9444449467669479, 0.32703080944632723, -1.4488030743375666)\n"
     ]
    }
   ],
   "source": [
    "img_path = SAVE_DIR +'/depth_.png'\n",
    "plane_est = ransac_plane_fitting(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_pose_obj(FOR_1, FOR_2, plane_model):\n",
    "    # Estimated Translation of table from camera coordinate\n",
    "    model_center = (FOR_1.get_center() + FOR_2.get_center())/2\n",
    "    FOR_origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    FOR_obj = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    FOR_obj.translate(model_center)\n",
    "    #o3d.visualization.draw_geometries([depth_pcd, FOR_origin, FOR_obj, FOR_1, FOR_2])\n",
    "\n",
    "    [a, b, c, d] = plane_model\n",
    "    \n",
    "    # z-axis of model(same to plane normal of ground)\n",
    "    ax_z = np.array([[a], [b], [c]])\n",
    "    if (b > 0):\n",
    "        ax_z = -np.array([[a], [b], [c]])\n",
    "\n",
    "    #a1 = np.array([[1], [0], [(-d-a)/c]])\n",
    "    #a2 = np.array([[0], [(-d-c)/b], [1]])\n",
    "    a1 = np.array([[1], [0], [(-d-a)/c]])\n",
    "    a2 = np.array([[0], [-1], [(-d+b)/c]])\n",
    "    A = np.asmatrix(np.hstack([a1, a2]))\n",
    "\n",
    "    # z1, z2를 내적 했는데, 양수면 그냥 하면 되고, 음수면 하나를 reverse해서 진행\n",
    "    z_axis = np.array([[0], [0], [1]])\n",
    "    z1 = np.asmatrix(ICP_result1[0:3, 0:3]) * np.asmatrix(z_axis)\n",
    "    z2 = np.asmatrix(ICP_result2[0:3, 0:3]) * np.asmatrix(z_axis)\n",
    "\n",
    "    if (np.dot(z1.T, z2) > 0):\n",
    "        # Projection to plane\n",
    "        ax_y1 = A * np.linalg.inv(A.T*A) * A.T * z1\n",
    "        ax_y2 = A * np.linalg.inv(A.T*A) * A.T * z2\n",
    "        ax_y = (ax_y1 + ax_y2)/2\n",
    "    else:\n",
    "        z2 = -z2\n",
    "        # Projection to plane\n",
    "        ax_y1 = A * np.linalg.inv(A.T*A) * A.T * z1\n",
    "        ax_y2 = A * np.linalg.inv(A.T*A) * A.T * z2\n",
    "        ax_y = (ax_y1 + ax_y2)/2\n",
    "\n",
    "\n",
    "    # Find last axis through cross product\n",
    "    ax_x = np.cross(ax_y.T, ax_z.T).T\n",
    "\n",
    "    # Normalize\n",
    "    ax_x = ax_x / np.linalg.norm(ax_x)\n",
    "    ax_y = ax_y / np.linalg.norm(ax_y)\n",
    "    ax_z = ax_z / np.linalg.norm(ax_z)\n",
    "    #ax_y = np.cross(ax_z.T, ax_x.T).T\n",
    "\n",
    "    # Estimated Orientation of table from camera coordinate\n",
    "    model_rotation_mtx = np.hstack([ax_x, ax_y, ax_z])\n",
    "    #print(model_rotation_mtx)\n",
    "    #print(np.dot(ax_x.T, ax_z))\n",
    "    #print(np.dot(ax_x.T, ax_y))\n",
    "    #print(np.dot(ax_y.T, ax_z))\n",
    "\n",
    "    #trans_z = np.array([0, 0, depth/2])\n",
    "    FOR_final = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    FOR_final.translate(model_center)\n",
    "    FOR_final.rotate(model_rotation_mtx)\n",
    "    #FOR_final.translate(trans_z, relative=True)\n",
    "    FOR_origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    #o3d.visualization.draw_geometries([depth_pcd, FOR_origin, FOR_final])\n",
    "\n",
    "    print(\"========== Found 6DoF Pose of table ==========\")\n",
    "    print(\"Rotation matrix is :\")\n",
    "    print(model_rotation_mtx)\n",
    "    print(\"\\nTranslation is :\")\n",
    "    print(model_center)\n",
    "    T_co = np.identity(4)\n",
    "    T_co[0:3,0:3] = model_rotation_mtx\n",
    "    T_co[0:3,3] = model_center.T\n",
    "    print(\"\\nHomogeneous Transformation(Cam to Table) is :\")\n",
    "    print(T_co)\n",
    "    \n",
    "    return T_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Found 6DoF Pose of table ==========\n",
      "Rotation matrix is :\n",
      "[[ 0.99743319 -0.06801297  0.03278097]\n",
      " [ 0.01007282 -0.17629035 -0.94444495]\n",
      " [ 0.07089119  0.98198572 -0.32703081]]\n",
      "\n",
      "Translation is :\n",
      "[-1.08995667 -0.26745295  3.88352824]\n",
      "\n",
      "Homogeneous Transformation(Cam to Table) is :\n",
      "[[ 0.99743319 -0.06801297  0.03278097 -1.08995667]\n",
      " [ 0.01007282 -0.17629035 -0.94444495 -0.26745295]\n",
      " [ 0.07089119  0.98198572 -0.32703081  3.88352824]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "T_co = first_pose_obj(FOR_1, FOR_2, plane_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneous Transformation(Base to Table) is :\n",
      "[[ 0.13507169 -0.92034153  0.50688383 -4.46558778]\n",
      " [-0.14647793  0.34782915  0.86001456  1.43100604]\n",
      " [-0.9799489  -0.17884747 -0.05868417  1.11447614]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Transformation from robot base to table\n",
    "T_bo = np.asmatrix(T_v) * np.asmatrix(T_co)\n",
    "print(\"Homogeneous Transformation(Base to Table) is :\")\n",
    "print(T_bo)\n",
    "center_p = T_bo[0:3,3].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneous Transformation(Mobile Base to Table) is :\n"
     ]
    }
   ],
   "source": [
    "# Transformation from mobile base to robot arm base (coordinate 알려줘야 적을듯)\n",
    "#T_mb = \n",
    "#T_mo = T_mb * T_bo\n",
    "print(\"Homogeneous Transformation(Mobile Base to Table) is :\")\n",
    "#print(T_mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Rotation matrix to roll-pitch-yaw(ZYX Euler angle)\n",
    "R = T_bo[0:3,0:3]\n",
    "sy = sqrt(R[0,0]**2 + R[1,0]**2)\n",
    "\n",
    "if sy > 0.000001:\n",
    "    x = atan2(R[2,1] , R[2,2])\n",
    "    y = atan2(-R[2,0], sy)\n",
    "    z = atan2(R[1,0], R[0,0])\n",
    "else:\n",
    "    x = atan2(-R[1,2], R[1,1])\n",
    "    y = atan2(-R[2,0], sy)\n",
    "    z = 0\n",
    "rpy_p = np.asarray(list(reversed(np.asarray([z,y,x]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometry 추가\n",
    "table = gscene.create_safe(gtype=GEOTYPE.BOX, name=\"table\", link_name=\"base_link\", \n",
    "                   dims=(0.785,1.80,0.735), center=center_p, rpy=rpy_p, \n",
    "                   color=(0.8,0.8,0.8,0.5), display=True, fixed=True, collision=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464.578125, 464.57421875, 351.5234375, 238.875)\n"
     ]
    }
   ],
   "source": [
    "# 모서리부분 근접 촬영\n",
    "_, _ = camera_streaming()\n",
    "print(cam_intrins.fx, cam_intrins.fy, cam_intrins.ppx, cam_intrins.ppy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_detect(img_path):\n",
    "    #Ransac Plane Fitting\n",
    "    #depth_img = cv2.imread('/home/jhkim/Swin-Transformer-Object-Detection/test_table.png',\n",
    "    #                                                               flags=cv2.IMREAD_UNCHANGED)\n",
    "    #depth_raw = o3d.io.read_image('/home/jhkim/Swin-Transformer-Object-Detection/test_table.png')\n",
    "    #depth_img = cv2.imread('/home/jhkim/Projects/rnb-planning/src/scripts/demo_202107/save_img/table.png',\n",
    "    #                                                              flags=cv2.IMREAD_UNCHANGED)\n",
    "    #depth_raw = o3d.io.read_image('/home/jhkim/Projects/rnb-planning/src/scripts/demo_202107/save_img/table.png')\n",
    "    depth_img = cv2.imread(img_path, flags=cv2.IMREAD_UNCHANGED)\n",
    "    depth_raw = o3d.io.read_image(img_path) \n",
    "    depth_pcd_raw= o3d.geometry.PointCloud.create_from_depth_image(depth_raw,\n",
    "                                        o3d.camera.PinholeCameraIntrinsic(cam_intrins.width,\n",
    "                                        cam_intrins.height,cam_intrins.fx, cam_intrins.fy,\n",
    "                                        cam_intrins.ppx, cam_intrins.ppy), depth_scale = 1/d_scale,\n",
    "                                                                                   depth_trunc = 1)\n",
    "    #depth_pcd_raw = o3d.geometry.PointCloud.create_from_depth_image(depth_raw, o3d.camera.PinholeCameraIntrinsic(640, 480, \n",
    "    #                                                                                                  461.734375, 462.06640625, \n",
    "    #                                                                                                  350.4140625, 244.541015625), depth_scale = 4000.0, depth_trunc = 1)\n",
    "\n",
    "\n",
    "    plane_model, inliers = depth_pcd_raw.segment_plane(distance_threshold=0.001,\n",
    "                                             ransac_n=8,\n",
    "                                             num_iterations=1000)\n",
    "    [a, b, c, d] = plane_model\n",
    "    print(\"Coeffs of eq of fitting plane are :\")\n",
    "    print(a,b,c,d)\n",
    "\n",
    "    p_inliers = depth_pcd_raw.points[0]\n",
    "    for i in range(len(inliers)-1):\n",
    "        p_inliers = np.vstack([p_inliers, depth_pcd_raw.points[i+1]])\n",
    "\n",
    "    pcd_inliers = o3d.geometry.PointCloud()\n",
    "    pcd_inliers.points = o3d.utility.Vector3dVector(p_inliers)\n",
    "    #o3d.visualization.draw_geometries([pcd_inliers])\n",
    "\n",
    "    # Generate Binary mask image to detect line and edge of table\n",
    "    depth_make = np.zeros([480,640], dtype='uint8')\n",
    "    for i in range(len(inliers)):\n",
    "        u = int((p_inliers[i][0]/p_inliers[i][2]) * cam_intrins.fx + cam_intrins.ppx)\n",
    "        v = int((p_inliers[i][1]/p_inliers[i][2]) * cam_intrins.fy + cam_intrins.ppy)\n",
    "        depth_make[v][u] = 255\n",
    "    #plt.imshow(depth_make)\n",
    "    \n",
    "    # Closing image\n",
    "    kernel = np.ones((11,11), np.uint8)\n",
    "    #depth_make_open = cv2.morphologyEx(depth_make, cv2.MORPH_OPEN, kernel)\n",
    "    depth_make_close = cv2.morphologyEx(depth_make, cv2.MORPH_CLOSE, kernel)\n",
    "    plt.imshow(depth_make_close)\n",
    "    \n",
    "    # Canny Edge Detection & Probabilistic Hough Line Detection\n",
    "    # Parameter(threshold value) shoud be tuned for our Demo Environment\n",
    "    tmp = cv2.cvtColor(depth_make_close, cv2.COLOR_GRAY2RGB)\n",
    "    canny = cv2.Canny(depth_make_close, 450, 1000, apertureSize = 5, L2gradient = True)\n",
    "    lines = cv2.HoughLinesP(canny, 1, np.pi/180, 50, minLineLength = 13, maxLineGap = 40)\n",
    "\n",
    "    # Check the line detection result\n",
    "    for i in lines:\n",
    "        cv2.line(tmp, (i[0][0], i[0][1]), (i[0][2], i[0][3]), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Line Detect\", tmp)\n",
    "    cv2.imshow(\"Canny\", canny)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return depth_img, lines, plane_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeffs of eq of fitting plane are :\n",
      "(-0.026615157292016602, 0.6225086441746293, 0.7821602274024076, -0.595054375584091)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = SAVE_DIR + '/table_.png'\n",
    "depth_img, lines, plane_est = line_detect(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity2line1(u_j, v_j, u_list, v_list):\n",
    "    u1 = u_list[0]\n",
    "    u2 = u_list[1]\n",
    "    v1 = v_list[0]\n",
    "    v2 = v_list[1]\n",
    "    value = (v_j-v1)(u1-u2) - (v1-v2)(u_j-u1)\n",
    "    return abs(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity2line2(u_j, v_j, u_list, v_list):\n",
    "    u1 = u_list[0]\n",
    "    u3 = u_list[2]\n",
    "    v1 = v_list[0]\n",
    "    v3 = v_list[2]\n",
    "    value = (v_j-v1)(u3-u1) - (v3-v1)(u_J-u1)\n",
    "    return abs(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 꼭짓점 4개의 좌표가 온다고 할 때,\n",
    "# 아니다.사진 찍은 위치의 코너를 알고 있으면 좌표 세개로부터 직선 두개 결정\n",
    "# stack하는 순서는 찍은 코너를 첫번째로 stacking\n",
    "# point 3개가 넘어온다 생각하고, pixel 좌표로 변환한 거를 저장하는 u,v list\n",
    "\n",
    "# test\n",
    "p1 = np.array([0.5,0.5,T_Depth])\n",
    "p2 = np.array([0.5,0.5+T_Width,T_Depth])\n",
    "p3 = np.array([0.5+T_Height,0.5,T_Depth])\n",
    "p = np.vstack([p1, p2, p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detect(depth_img, p):\n",
    "    # Convert points w.r.t pixel coordinate\n",
    "    u_list = []\n",
    "    v_list = []\n",
    "    for i in range(3):\n",
    "        u_list.append(int((p[i][0]/p[i][2]) * cam_intrins.fx + cam_intrins.ppx))\n",
    "        v_list.append(int((p[i][1]/p[i][2]) * cam_intrins.fy + cam_intrins.ppy))\n",
    "\n",
    "    # Find similar lines\n",
    "    metric1 = []\n",
    "    metric2 = []\n",
    "    for i in lines:\n",
    "        xe, ye, xs, ys = i[0][0], i[0][1], i[0][2], i[0][3]\n",
    "        result1 = []\n",
    "        result2 = []\n",
    "        for u_j in range(xs, xe):\n",
    "            v_j = (ye-ys)/(xe-xs) * (u_j-xs) + ys \n",
    "            #u_j = int((xe-xs)/(ye-ys) * (v_j-ys) + xs)\n",
    "            result1.append(similarity2line1(u_j, v_j, u_list, v_list))\n",
    "            result2.append(similarity2line2(u_j, v_j, u_list, v_list))\n",
    "\n",
    "        metric1.append(np.mean(result1))\n",
    "        metric2.append(np.mean(result2))\n",
    "\n",
    "    index1 = metric1.index(min(metric1))\n",
    "    index2 = metric2.index(min(metric2))\n",
    "    #print(index1, index2)\n",
    "\n",
    "\n",
    "    # Start, End Points of lines\n",
    "    xe, ye, xs, ys = lines[index1][0]\n",
    "    xe_, ye_, xs_, ys_ = lines[index2][0]\n",
    "    #print(xe, ye, xs, ys)\n",
    "    #print(xe_, ye_, xs_, ys_)\n",
    "\n",
    "    # Two vectors which represent the line\n",
    "    z1 = (depth_img[ys][xs]/65535.0)/(d_scale*1000)\n",
    "    x1 = (xs - cam_intrins.ppx) * z1/cam_intrins.fx\n",
    "    y1 = (ys - cam_intrins.ppy) * z1/cam_intrins.fy\n",
    "    p1 = np.asmatrix(np.array([[x1],[y1],[z1]]))\n",
    "\n",
    "    z2 = (depth_img[ye][xe]/65535.0)/(d_scale*1000)\n",
    "    x2 = (xe - cam_intrins.ppx) * z2/cam_intrins.fx\n",
    "    y2 = (ye - cam_intrins.ppy) * z2/cam_intrins.fy\n",
    "    p2 = np.asmatrix(np.array([[x2],[y2],[z2]]))\n",
    "\n",
    "    z1_ = (depth_img[ys_][xs_]/65535.0)/(d_scale*1000)\n",
    "    x1_ = (xs_ - cam_intrins.ppx) * z1_/cam_intrins.fx\n",
    "    y1_ = (ys_ - cam_intrins.ppy) * z1_/cam_intrins.fy\n",
    "    p1_ = np.asmatrix(np.array([[x1_],[y1_],[z1_]]))\n",
    "\n",
    "    z2_ = (depth_img[ye_][xe_]/65535.0)/(d_scale*1000)\n",
    "    x2_ = (xs_ - cam_intrins.ppx) * z2_/cam_intrins.fx\n",
    "    y2_ = (ys_ - cam_intrins.ppy) * z2_/cam_intrins.fy\n",
    "    p2_ = np.asmatrix(np.array([[x2_],[y2_],[z2_]]))\n",
    "\n",
    "    line_vec1 = p2 - p1\n",
    "    line_vec2 = p2_ - p1_\n",
    "    ax_x = line_vec1\n",
    "    ax_y = line_vec2\n",
    "\n",
    "    # Detect Edge point of table\n",
    "    edge_cand1 = (p1+p1_)/2\n",
    "    edge_cand2 = (p1+p2_)/2\n",
    "    edge_cand3 = (p2+p1_)/2\n",
    "    edge_cand4 = (p2+p2_)/2\n",
    "\n",
    "    dist_cand1 = np.linalg.norm(p1 - p1_)\n",
    "    dist_cand2 = np.linalg.norm(p1 - p2_)\n",
    "    dist_cand3 = np.linalg.norm(p2 - p1_)\n",
    "    dist_cand4 = np.linalg.norm(p2 - p2_)\n",
    "\n",
    "    edge_list = [dist_cand1, dist_cand2, dist_cand3, dist_cand4]\n",
    "    idx = edge_list.index(min(edge_list)) + 1\n",
    "\n",
    "    edge = edge_cand4\n",
    "    if (idx == 1):\n",
    "        edge = edge_cand1\n",
    "\n",
    "    if (idx == 2):\n",
    "        edge = edge_cand2\n",
    "\n",
    "    if (idx == 3):\n",
    "        edge = edge_cand3\n",
    "\n",
    "    if (idx == 4):\n",
    "        edge = edge_cand4\n",
    "    \n",
    "    return edge, ax_x, ax_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "edge, ax_x, ax_y = edge_detect(depth_img, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Testing용 #####\n",
    "T_co_new = np.identity(4)\n",
    "T_co_new[0:3,0] = np.array([0,1,0]).T\n",
    "T_co_new[0:3,1] = np.array([-1,0,0]).T\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_obj(plane_model, edge, ax_x, ax_y, T_co_new):\n",
    "    a, b, c, d = plane_model\n",
    "    surface_normal = np.array([[a], [b], [c]])\n",
    "    #surface_normal = np.cross(line_v1.T, line_v2.T) \n",
    "\n",
    "    # ax_x, ax_y가 line으로부터 땄는데, 반드시 수직하다는 보장 있나..? 근접샷에서 딴 거니까 정확하다고 그냥 생각해야하나\n",
    "    # normalize \n",
    "    if (np.dot(T_co_new[0:3,0].T, ax_x) < 0):\n",
    "        ax_x = -ax_x\n",
    "\n",
    "    if (np.dot(T_co_new[0:3,1].T, ax_y) < 0):\n",
    "        ax_y = -ax_y\n",
    "\n",
    "    ax_z = surface_normal/np.linalg.norm(surface_normal)\n",
    "    ax_x = ax_x/np.linalg.norm(ax_x)\n",
    "    ax_y = ax_y/np.linalg.norm(ax_y)\n",
    "    #print(np.dot(ax_z.T,ax_x))\n",
    "    #print(np.dot(ax_x.T,ax_y))\n",
    "    #print(np.dot(ax_y.T,ax_z))\n",
    "\n",
    "\n",
    "    # Estimated Orientation of table from camera coordinate\n",
    "    model_rotation_mtx = np.hstack([ax_x, ax_y, ax_z])\n",
    "\n",
    "    #estimated_box = o3d.geometry.TriangleMesh.create_box(width=T_Width, height=T_Height, depth=T_Depth)\n",
    "    #estimated_box.translate(edge)\n",
    "    #estimated_box.rotate(model_rotation_mtx)\n",
    "    FOR_box = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.15, origin=[0,0,0])\n",
    "    FOR_box.translate(edge)\n",
    "    FOR_box.rotate(model_rotation_mtx)\n",
    "    #FOR_box.translate((-T_Height/2, T_Width/2, -T_Depth/2), relative=True)\n",
    "\n",
    "    T_co_result = np.identity(4)\n",
    "    T_co_result[0:3,0:3] = model_rotation_mtx\n",
    "    T_co_result[0:3,3] = FOR_box.get_center().T\n",
    "    #model_new_center = (T_co_new[0:3,3] + FOR_box.get_center())/2\n",
    "\n",
    "    print(\"\\nHomogeneous Transformation(Cam to Table) is :\")\n",
    "    print(T_co_result)\n",
    "    return T_co_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Homogeneous Transformation(Cam to Table) is :\n",
      "[[-0.99931653 -0.52512772 -0.02661516  0.13276524]\n",
      " [ 0.00276719  0.063706    0.62250864 -0.00741477]\n",
      " [-0.03686213 -0.84863562  0.78216023  0.20978076]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "T_co_result = pose_obj(plane_est, edge, ax_x, ax_y, T_co_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
