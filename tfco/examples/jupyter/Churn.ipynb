{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we explore the problem of training classifier to reducing churn. That is, given that we've trained a model, how do we train another model so that the predictions don't differ much by the previous model. We show here that training for churn reduction may actually help improve accuracy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from six.moves import xrange\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and processing dataset.\n",
    "\n",
    "We load the [[UCI Adult dataset]](https://archive.ics.uci.edu/ml/datasets/adult) and do some pre-processing. The dataset is based on census data and the goal is to predict whether someone's income is over 50k.\n",
    "\n",
    "We preprocess the features as done in works such as [[ZafarEtAl15]](https://arxiv.org/abs/1507.05259) and [[GohEtAl16]](https://arxiv.org/abs/1606.07558). We transform the categorical features into binary ones and transform the continuous feature into buckets based on each feature's 5 quantiles values in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'gender', 'native_country'\n",
    "]\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "]\n",
    "COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "LABEL_COLUMN = 'label'\n",
    "CHURN_COLUMN = 'churn_label'\n",
    "\n",
    "def get_data():\n",
    "    train_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=COLUMNS, skipinitialspace=True)\n",
    "    test_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "\n",
    "    train_df_raw[LABEL_COLUMN] = (train_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    test_df_raw[LABEL_COLUMN] = (test_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    # Preprocessing Features\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Functions for preprocessing categorical and continuous columns.\n",
    "    def binarize_categorical_columns(input_train_df, input_test_df, categorical_columns=[]):\n",
    "\n",
    "        def fix_columns(input_train_df, input_test_df):\n",
    "            test_df_missing_cols = set(input_train_df.columns) - set(input_test_df.columns)\n",
    "            for c in test_df_missing_cols:\n",
    "                input_test_df[c] = 0\n",
    "                train_df_missing_cols = set(input_test_df.columns) - set(input_train_df.columns)\n",
    "            for c in train_df_missing_cols:\n",
    "                input_train_df[c] = 0\n",
    "                input_train_df = input_train_df[input_test_df.columns]\n",
    "            return input_train_df, input_test_df\n",
    "\n",
    "        # Binarize categorical columns.\n",
    "        binarized_train_df = pd.get_dummies(input_train_df, columns=categorical_columns)\n",
    "        binarized_test_df = pd.get_dummies(input_test_df, columns=categorical_columns)\n",
    "        # Make sure the train and test dataframes have the same binarized columns.\n",
    "        fixed_train_df, fixed_test_df = fix_columns(binarized_train_df, binarized_test_df)\n",
    "        return fixed_train_df, fixed_test_df\n",
    "\n",
    "    def bucketize_continuous_column(input_train_df,\n",
    "                                  input_test_df,\n",
    "                                  continuous_column_name,\n",
    "                                  num_quantiles=None,\n",
    "                                  bins=None):\n",
    "        assert (num_quantiles is None or bins is None)\n",
    "        if num_quantiles is not None:\n",
    "            train_quantized, bins_quantized = pd.qcut(\n",
    "              input_train_df[continuous_column_name],\n",
    "              num_quantiles,\n",
    "              retbins=True,\n",
    "              labels=False)\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins_quantized, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
    "        elif bins is not None:\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins, labels=False)\n",
    "\n",
    "    # Filter out all columns except the ones specified.\n",
    "    train_df = train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    test_df = test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    \n",
    "    # Bucketize continuous columns.\n",
    "    bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
    "    bucketize_continuous_column(train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
    "    bucketize_continuous_column(train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
    "    train_df, test_df = binarize_categorical_columns(train_df, test_df, categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
    "    feature_names = list(train_df.keys())\n",
    "    feature_names.remove(LABEL_COLUMN)\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    return train_df, test_df, feature_names\n",
    "\n",
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.\n",
    "\n",
    "We will use a simple neural network model as the initial classifier. Then we train a linear model with churn constraints to ensure that the linear model's predictions don't differ by much from that of the neural network.\n",
    "\n",
    "In the following code, we initialize the placeholders and model. In build_train_op, we set up the constrained optimization problem. We create a rate context for the entire dataset to get the error rate on the training data with respect to the labels. We then create a separate rate context to calculate the error rate on the training data with respect to the outputs of the initial model. We then construct a minimization problem using RateMinimizationProblem and use the LagrangianOptimizerV1 as the solver. build_train_op initializes a training operation which will later be used to actually train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _construct_model(input_tensor, hidden_units=None):\n",
    "    hidden = input_tensor\n",
    "    if hidden_units:\n",
    "        hidden = tf.layers.dense(\n",
    "            inputs=input_tensor,\n",
    "            units=hidden_units,\n",
    "            activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(\n",
    "        inputs=hidden,\n",
    "        units=1,\n",
    "        activation=None)\n",
    "    return output\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 hidden_units=None,\n",
    "                 max_churn_rate=0.05):\n",
    "        tf.random.set_random_seed(123)\n",
    "        self.max_churn_rate = max_churn_rate\n",
    "        num_features = len(FEATURE_NAMES)\n",
    "        self.features_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, num_features), name='features_placeholder')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, 1), name='labels_placeholder')\n",
    "        self.churn_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, 1), name='churn_placeholder')\n",
    "        # We use a linear model.\n",
    "        self.predictions_tensor = _construct_model(self.features_placeholder, hidden_units=hidden_units)\n",
    "\n",
    "\n",
    "    def build_train_op(self,\n",
    "                       learning_rate,\n",
    "                       train_with_churn=False):\n",
    "        ctx = tfco.rate_context(self.predictions_tensor, self.labels_placeholder)\n",
    "        ctx_churn = tfco.rate_context(self.predictions_tensor, self.churn_placeholder)\n",
    "        constraints = [tfco.error_rate(ctx_churn) <= self.max_churn_rate] if train_with_churn else []\n",
    "        mp = tfco.RateMinimizationProblem(tfco.error_rate(ctx), constraints)\n",
    "        opt = tfco.LagrangianOptimizerV1(tf.train.AdamOptimizer(learning_rate))\n",
    "        self.train_op = opt.minimize(mp)\n",
    "        return self.train_op\n",
    "  \n",
    "    def feed_dict_helper(self, dataframe):\n",
    "        return {self.features_placeholder:\n",
    "                  dataframe[FEATURE_NAMES],\n",
    "                self.labels_placeholder:\n",
    "                  dataframe[[LABEL_COLUMN]],\n",
    "                self.churn_placeholder: dataframe[[CHURN_COLUMN]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training.\n",
    "\n",
    "Below is the function which performs the training of our constrained optimization problem. Each call to the function does one epoch through the dataset and then yields the training and testing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(model,\n",
    "                       train_df,\n",
    "                       test_df,\n",
    "                       minibatch_size,\n",
    "                       num_iterations_per_loop=1,\n",
    "                       num_loops=1):\n",
    "    random.seed(31337)\n",
    "    num_rows = train_df.shape[0]\n",
    "    minibatch_size = min(minibatch_size, num_rows)\n",
    "    permutation = list(range(train_df.shape[0]))\n",
    "    random.shuffle(permutation)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run((tf.global_variables_initializer(),\n",
    "               tf.local_variables_initializer()))\n",
    "\n",
    "    minibatch_start_index = 0\n",
    "    for n in xrange(num_loops):\n",
    "        for _ in xrange(num_iterations_per_loop):\n",
    "            minibatch_indices = []\n",
    "            while len(minibatch_indices) < minibatch_size:\n",
    "                minibatch_end_index = (\n",
    "                minibatch_start_index + minibatch_size - len(minibatch_indices))\n",
    "                if minibatch_end_index >= num_rows:\n",
    "                    minibatch_indices += range(minibatch_start_index, num_rows)\n",
    "                    minibatch_start_index = 0\n",
    "                else:\n",
    "                    minibatch_indices += range(minibatch_start_index, minibatch_end_index)\n",
    "                    minibatch_start_index = minibatch_end_index\n",
    "                    \n",
    "            session.run(\n",
    "                  model.train_op,\n",
    "                  feed_dict=model.feed_dict_helper(\n",
    "                      train_df.iloc[[permutation[ii] for ii in minibatch_indices]]))\n",
    "\n",
    "        train_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        test_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(test_df))\n",
    "\n",
    "        yield (train_predictions, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing accuracy and constraint metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    signed_labels = (\n",
    "      (labels > 0).astype(np.float32) - (labels <= 0).astype(np.float32))\n",
    "    numerator = (np.multiply(signed_labels, predictions) <= 0).sum()\n",
    "    denominator = predictions.shape[0]\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def _get_error_rate_and_constraints(df, max_churn_rate):\n",
    "    \"\"\"Computes the error and constraint violations.\"\"\"\n",
    "    error_rate_local = error_rate(df[['predictions']], df[[LABEL_COLUMN]])\n",
    "    error_rate_churn = error_rate(df[['predictions']], df[[CHURN_COLUMN]])\n",
    "    return error_rate_local, error_rate_churn - max_churn_rate\n",
    "\n",
    "def training_helper(model,\n",
    "                    train_df,\n",
    "                    test_df,\n",
    "                    minibatch_size,\n",
    "                    num_iterations_per_loop=1,\n",
    "                    num_loops=1):\n",
    "    train_error_rate_vector = []\n",
    "    train_constraints_matrix = []\n",
    "    test_error_rate_vector = []\n",
    "    test_constraints_matrix = []\n",
    "    for train, test in training_generator(\n",
    "      model, train_df, test_df, minibatch_size, num_iterations_per_loop,\n",
    "      num_loops):\n",
    "        train_df['predictions'] = train\n",
    "        test_df['predictions'] = test\n",
    "\n",
    "        train_error_rate, train_constraints = _get_error_rate_and_constraints(\n",
    "          train_df, model.max_churn_rate)\n",
    "        train_error_rate_vector.append(train_error_rate)\n",
    "        train_constraints_matrix.append(train_constraints)\n",
    "\n",
    "        test_error_rate, test_constraints = _get_error_rate_and_constraints(\n",
    "            test_df, model.max_churn_rate)\n",
    "        test_error_rate_vector.append(test_error_rate)\n",
    "        test_constraints_matrix.append(test_constraints)\n",
    "\n",
    "    return (train_error_rate_vector, train_constraints_matrix, test_error_rate_vector, test_constraints_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network without constraints\n",
    "\n",
    "We train a neural network with 10 hidden units to obtain a baseline model in which we can train for churn for subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_units=10)\n",
    "model.build_train_op(0.01, train_with_churn=False)\n",
    "\n",
    "# initialize the labels for churn reduction to the true labels as placeholder.\n",
    "train_df[CHURN_COLUMN] = train_df[LABEL_COLUMN]\n",
    "test_df[CHURN_COLUMN] = test_df[LABEL_COLUMN]\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.13086207426061852\n",
      "Test Error 0.14120754253424236\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Test Error\", test_errors[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Network Predictions as examples to reduce churn against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[CHURN_COLUMN] = train_df[\"predictions\"]\n",
    "test_df[CHURN_COLUMN] = test_df[\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline without constraints.\n",
    "\n",
    "We now declare the model, build the training op, and then perform the training. We use a linear classifier, and train using the ADAM optimizer with learning rate 0.01, with minibatch size of 100 over 100 epochs. We first train without churn constraints to show the baseline performance. We see that without training for churn, we obtain some churn violation.\n",
    "\n",
    "We also see that unsurprisingly, the performance of the linear model is considerably worse than that of the neural network model in both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_units=None, max_churn_rate=0.025)\n",
    "model.build_train_op(0.01, train_with_churn=False)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14296243972850955\n",
      "Train Violation 0.01940895549890974\n",
      "\n",
      "Test Error 0.1428659173269455\n",
      "Test Violation 0.01879337878508691\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", train_violations[-1])\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", test_violations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with churn constraints.\n",
    "\n",
    "We now train our linear model with churn constraints so that the linear model's predictions don't differ from that of the neural network by too much. We set the threshold to 0.025 so that the goal is to train for accuracy while ensuring that we only deviate from the network outputs by 2.5%.\n",
    "\n",
    "Interestingly, not only do we get very close to succeeding in enforcing this churn constraint, we see that the overall accuracy of the linear model improves when compared to training the linear model without the churn constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_units=None, max_churn_rate=0.025)\n",
    "model.build_train_op(0.01, train_with_churn=True)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.13906206811830105\n",
      "Train Violation 0.000767021897361872\n",
      "\n",
      "Test Error 0.14102327866838646\n",
      "Test Violation 0.0029466863214790244\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", train_violations[-1])\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", test_violations[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
