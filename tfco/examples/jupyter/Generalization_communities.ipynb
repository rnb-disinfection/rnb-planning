{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we explore the problem of fairness generalization. That is, given that we've trained a model to satisfy the fairness constraints on training data, will fairness also hold on testing. We present the approach proposed by [[CotterEtAl2018]](https://arxiv.org/abs/1807.00028). Constrained optimization can be viewed as a two-player game where one player minimizes the training error and the second player minimizes the constraint violation. The main idea of [[CotterEtAl2018]](https://arxiv.org/abs/1807.00028) is that if each player optimizes over the same dataset, then the fairness generalization error will have both the error due to the model complexity and the constraints and a better approach is for each player to use separate data. For example, as we will show in this notebook, we can allow the first player to use the first half of the training dataset and the second player to use the second half of the training dataset. Since the two players see different datasets, the generalization guarantees for fairness will be decoupled from the model complexity and thus can be substantially improved, especially when the model is complex. We show that this is indeed the case on the [[Communities and Crime dataset]](http://archive.ics.uci.edu/ml/datasets/communities+and+crime). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from six.moves import xrange\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and processing dataset.\n",
    "\n",
    "We load and download the [[Communities and Crime dataset]](http://archive.ics.uci.edu/ml/datasets/communities+and+crime) and do some pre-processing. We impute the missing values for each feature by the average of that feature over the training set. We then construct eight protected groups, two for each race based on whether the percentage of that race in a community was above or below median. We also convert the continuous label to a binary one.\n",
    "\n",
    "The fairness goal is to make sure that the rate at which we falsely predict a neighborhood is violent for any of the protected groups is no higher than that of the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = \"state,county,community,communityname,fold,population,householdsize,racePctblack,racePctWhite,racePctAsian,racePctHisp,agePct12t21,agePct12t29,agePct16t24,agePct65up,numbUrban,pctUrban,medIncome,pctWWage,pctWFarmSelf,pctWInvInc,pctWSocSec,pctWPubAsst,pctWRetire,medFamInc,perCapInc,whitePerCap,blackPerCap,indianPerCap,AsianPerCap,OtherPerCap,HispPerCap,NumUnderPov,PctPopUnderPov,PctLess9thGrade,PctNotHSGrad,PctBSorMore,PctUnemployed,PctEmploy,PctEmplManu,PctEmplProfServ,PctOccupManu,PctOccupMgmtProf,MalePctDivorce,MalePctNevMarr,FemalePctDiv,TotalPctDiv,PersPerFam,PctFam2Par,PctKids2Par,PctYoungKids2Par,PctTeen2Par,PctWorkMomYoungKids,PctWorkMom,NumIlleg,PctIlleg,NumImmig,PctImmigRecent,PctImmigRec5,PctImmigRec8,PctImmigRec10,PctRecentImmig,PctRecImmig5,PctRecImmig8,PctRecImmig10,PctSpeakEnglOnly,PctNotSpeakEnglWell,PctLargHouseFam,PctLargHouseOccup,PersPerOccupHous,PersPerOwnOccHous,PersPerRentOccHous,PctPersOwnOccup,PctPersDenseHous,PctHousLess3BR,MedNumBR,HousVacant,PctHousOccup,PctHousOwnOcc,PctVacantBoarded,PctVacMore6Mos,MedYrHousBuilt,PctHousNoPhone,PctWOFullPlumb,OwnOccLowQuart,OwnOccMedVal,OwnOccHiQuart,RentLowQ,RentMedian,RentHighQ,MedRent,MedRentPctHousInc,MedOwnCostPctInc,MedOwnCostPctIncNoMtg,NumInShelters,NumStreet,PctForeignBorn,PctBornSameState,PctSameHouse85,PctSameCity85,PctSameState85,LemasSwornFT,LemasSwFTPerPop,LemasSwFTFieldOps,LemasSwFTFieldPerPop,LemasTotalReq,LemasTotReqPerPop,PolicReqPerOffic,PolicPerPop,RacialMatchCommPol,PctPolicWhite,PctPolicBlack,PctPolicHisp,PctPolicAsian,PctPolicMinor,OfficAssgnDrugUnits,NumKindsDrugsSeiz,PolicAveOTWorked,LandArea,PopDens,PctUsePubTrans,PolicCars,PolicOperBudg,LemasPctPolicOnPatr,LemasGangUnitDeploy,LemasPctOfficDrugUn,PolicBudgPerPop,ViolentCrimesPerPop\".split(\",\")\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\",  header=None, names=column_names, na_values=['?'])\n",
    "\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'racePctblack_cat', 'racePctAsian_cat', 'racePctWhite_cat',\n",
    "    'racePctHisp_cat'\n",
    "]\n",
    "\n",
    "\n",
    "PROTECTED_COLUMNS = ['racePctblack_cat_low', 'racePctblack_cat_high',\n",
    "                     'racePctAsian_cat_low', 'racePctAsian_cat_high',\n",
    "                     'racePctWhite_cat_low', 'racePctWhite_cat_high',\n",
    "                     'racePctHisp_cat_low', 'racePctHisp_cat_high']\n",
    "\n",
    "\n",
    "CONTINUOUS_LABEL_COLUMN = 'ViolentCrimesPerPop'\n",
    "\n",
    "BINARY_LABEL_COLUMN = 'label'\n",
    "\n",
    "EXCLUDED_COLUMNS = [\n",
    "    'state', 'county', 'community', 'communityname', 'ViolentCrimesPerPop'\n",
    "]\n",
    "\n",
    "def _train_test_split(df, test_frac=0.33, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    n = len(df)\n",
    "\n",
    "    test_end = int(test_frac * n)\n",
    "    test_df = df.iloc[perm[:test_end]]\n",
    "    train_df = df.iloc[perm[test_end:]]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_binary_label(input_df):\n",
    "    quantile = input_df[CONTINUOUS_LABEL_COLUMN].quantile(0.7)\n",
    "    input_df[BINARY_LABEL_COLUMN] = input_df.apply(\n",
    "      lambda row: 1 if row[CONTINUOUS_LABEL_COLUMN] > quantile else 0, axis=1)\n",
    "\n",
    "\n",
    "def add_protected_categories(input_df):\n",
    "    # Bucketize race percentages\n",
    "    input_df['racePctblack_cat'] = pd.qcut(\n",
    "      input_df['racePctblack'], 2, labels=['low', 'high'])\n",
    "    input_df['racePctAsian_cat'] = pd.qcut(\n",
    "      input_df['racePctAsian'], 2, labels=['low', 'high'])\n",
    "    input_df['racePctWhite_cat'] = pd.qcut(\n",
    "      input_df['racePctWhite'], 2, labels=['low', 'high'])\n",
    "    input_df['racePctHisp_cat'] = pd.qcut(\n",
    "      input_df['racePctHisp'], 2, labels=['low', 'high'])\n",
    "\n",
    "add_binary_label(df)\n",
    "add_protected_categories(df)\n",
    "df = pd.get_dummies(df, columns=CATEGORICAL_COLUMNS)\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "      name for name in df.keys()\n",
    "      if name not in [LABEL_COLUMN] + EXCLUDED_COLUMNS\n",
    "  ]\n",
    "\n",
    "\n",
    "train_df, test_df = _train_test_split(df)\n",
    "\n",
    "for column in FEATURE_NAMES:\n",
    "    train_mean = train_df[column].mean()\n",
    "    train_df[column].fillna(train_mean, inplace=True)\n",
    "    test_df[column].fillna(train_mean, inplace=True)\n",
    "\n",
    "np.random.seed(12345)\n",
    "train_df['SPLIT_0'] = np.random.randint(0, 2, train_df.shape[0])\n",
    "train_df['SPLIT_1'] = train_df['SPLIT_0'].apply(lambda row: 1 - row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.\n",
    "\n",
    "We use a 1-hidden layer Neural Network with ReLU activations and 10 hidden units. We show that even with a relatively simple model, we still can see substantial improvements in fairness generalization performance when using the approach of dataset splitting.\n",
    "\n",
    "In the following code, we initialize the placeholders and model. In build_train_op, we set up the constrained optimization problem. We create a rate context for the entire dataset, and compute the overall false positive rate as the positive prediction rate on the negatively labeled subset. We then construct a constraint for each of the protected groups based on the difference between the false positive rates of the protected group and that of the overall dataset.\n",
    "\n",
    "For the non-split approach, we use the typical rate_context, but for the split dataset approach, we use split_rate_context which allows us to conveniently input two separate datasets and then later in optimization, it will know which examples to use for the objective and which examples to use for the constraint.\n",
    "\n",
    "We then construct a minimization problem using RateMinimizationProblem and use the ProxyLagrangianOptimizerV1 as the solver. build_train_op initializes a training operation which will later be used to actually train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _construct_model(model_name, input_tensor, hidden_units=10):\n",
    "    with tf.variable_scope('model_name', reuse=True):\n",
    "        hidden = tf.layers.dense(\n",
    "            inputs=input_tensor,\n",
    "            units=hidden_units,\n",
    "            activation=tf.nn.relu,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            name=model_name + \"_hidden\")\n",
    "        output = tf.layers.dense(\n",
    "            inputs=hidden,\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "            name=model_name + \"_outputs\")\n",
    "        return output\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                feature_names,\n",
    "                hidden_units=10,\n",
    "                gen_split=False,\n",
    "                fpr_max_diff=0):\n",
    "        tf.random.set_random_seed(123)\n",
    "        self.feature_names = feature_names\n",
    "        self.fpr_max_diff = fpr_max_diff\n",
    "        num_features = len(self.feature_names)\n",
    "        self.gen_split = gen_split\n",
    "        if self.gen_split:\n",
    "            self.features_split_0 = tf.placeholder(\n",
    "                tf.float32, shape=(None, num_features), name='split_0_features_placeholder')\n",
    "            self.features_split_1 = tf.placeholder(\n",
    "                tf.float32, shape=(None, num_features), name='split_1_features_placeholder')\n",
    "            self.split_0_labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, 1), name='split_0_labels_placeholder')\n",
    "            self.split_1_labels = tf.placeholder(\n",
    "                tf.float32, shape=(None, 1), name='split_1_labels_placeholder')\n",
    "            self.split_0_predictions = _construct_model(\n",
    "                model_name, self.features_split_0, hidden_units=hidden_units)\n",
    "            self.split_1_predictions = _construct_model(\n",
    "                model_name, self.features_split_1, hidden_units=hidden_units)\n",
    "            self.protected_split_0 = [tf.placeholder(tf.float32, shape=(None, 1), name=attribute+\"_placeholder0\") for attribute in PROTECTED_COLUMNS]\n",
    "            self.protected_split_1 = [tf.placeholder(tf.float32, shape=(None, 1), name=attribute+\"_placeholder1\") for attribute in PROTECTED_COLUMNS]\n",
    "\n",
    "\n",
    "        self.features_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, num_features), name='features_placeholder')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, 1), name='labels_placeholder')\n",
    "        self.predictions_tensor = _construct_model(\n",
    "            model_name, self.features_placeholder, hidden_units=hidden_units)\n",
    "        self.protected_placeholders = [tf.placeholder(tf.float32, shape=(None, 1), name=attribute+\"_placeholder\") for attribute in PROTECTED_COLUMNS]\n",
    "\n",
    "    def build_train_op(self,\n",
    "                       learning_rate,\n",
    "                       unconstrained=False):\n",
    "        if self.gen_split:\n",
    "            ctx = tfco.split_rate_context(self.split_0_predictions, self.split_1_predictions, self.split_0_labels, self.split_1_labels)\n",
    "            negative_slice = ctx.subset(self.split_0_labels <= 0, self.split_1_labels <= 0) \n",
    "        else:\n",
    "            ctx = tfco.rate_context(self.predictions_tensor, self.labels_placeholder)\n",
    "            negative_slice = ctx.subset(self.labels_placeholder <= 0)\n",
    "            \n",
    "        overall_fpr = tfco.positive_prediction_rate(negative_slice)\n",
    "        constraints = []\n",
    "        if not unconstrained:\n",
    "            for i in range(len(PROTECTED_COLUMNS)):\n",
    "                if self.gen_split:\n",
    "                    slice_fpr = tfco.positive_prediction_rate(\n",
    "                        ctx.subset(\n",
    "                            (self.split_0_labels < 0) & (self.protected_split_0[i] > 0),\n",
    "                            (self.split_1_labels < 0) & (self.protected_split_1[i] > 0)))\n",
    "\n",
    "                else:\n",
    "                    slice_fpr = tfco.positive_prediction_rate(\n",
    "                        ctx.subset((self.protected_placeholders[i] > 0) & (self.labels_placeholder < 0)))\n",
    "                constraints.append(slice_fpr <= overall_fpr + self.fpr_max_diff)\n",
    "          \n",
    "        error = tfco.error_rate(ctx)\n",
    "        mp = tfco.RateMinimizationProblem(error, constraints)\n",
    "        opt = tfco.ProxyLagrangianOptimizerV1(tf.train.AdamOptimizer(learning_rate))\n",
    "        self.train_op = opt.minimize(mp)\n",
    "        return self.train_op\n",
    "  \n",
    "    def feed_dict_helper(self, dataframe, train=False):\n",
    "        feed_dict = {}\n",
    "        if self.gen_split and train:\n",
    "            feed_dict[self.features_split_0] = dataframe[dataframe['SPLIT_0'] > 0][self.feature_names]\n",
    "            feed_dict[self.features_split_1] = dataframe[dataframe['SPLIT_1'] > 0][self.feature_names]\n",
    "            feed_dict[self.split_0_labels] = dataframe[dataframe['SPLIT_0'] > 0][[LABEL_COLUMN]]\n",
    "            feed_dict[self.split_1_labels] = dataframe[dataframe['SPLIT_1'] > 0][[LABEL_COLUMN]]\n",
    "            for i, protected_attribute in enumerate(PROTECTED_COLUMNS):\n",
    "                feed_dict[self.protected_split_0[i]] = dataframe[dataframe['SPLIT_0'] > 0][[protected_attribute]]\n",
    "                feed_dict[self.protected_split_1[i]] = dataframe[dataframe['SPLIT_1'] > 0][[protected_attribute]]\n",
    "\n",
    "        elif self.gen_split and not train:\n",
    "            feed_dict[self.features_split_0] = dataframe[self.feature_names]\n",
    "            feed_dict[self.features_split_1] = dataframe[self.feature_names]\n",
    "            feed_dict[self.split_0_labels] = dataframe[[LABEL_COLUMN]]\n",
    "            feed_dict[self.split_1_labels] = dataframe[[LABEL_COLUMN]]\n",
    "            for i, protected_attribute in enumerate(PROTECTED_COLUMNS):\n",
    "                feed_dict[self.protected_split_0[i]] = dataframe[[protected_attribute]]\n",
    "                feed_dict[self.protected_split_1[i]] = dataframe[[protected_attribute]]\n",
    "\n",
    "        feed_dict[self.features_placeholder] = dataframe[self.feature_names]\n",
    "        feed_dict[self.labels_placeholder] = dataframe[[LABEL_COLUMN]]\n",
    "        for i, protected_attribute in enumerate(PROTECTED_COLUMNS):\n",
    "            feed_dict[self.protected_placeholders[i]] = dataframe[[protected_attribute]]\n",
    "            \n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training.\n",
    "\n",
    "Below is the function which performs the training of our constrained optimization problem. Each call to the function does one epoch through the dataset and then yields the training and testing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(model,\n",
    "                       train_df,\n",
    "                       test_df,\n",
    "                       minibatch_size,\n",
    "                       num_iterations_per_loop=1,\n",
    "                       num_loops=1):\n",
    "    random.seed(31337)\n",
    "    num_rows = train_df.shape[0]\n",
    "    minibatch_size = min(minibatch_size, num_rows)\n",
    "    permutation = list(range(train_df.shape[0]))\n",
    "    random.shuffle(permutation)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run((tf.global_variables_initializer(),\n",
    "               tf.local_variables_initializer()))\n",
    "\n",
    "    minibatch_start_index = 0\n",
    "    for n in xrange(num_loops):\n",
    "        for _ in xrange(num_iterations_per_loop):\n",
    "            minibatch_indices = []\n",
    "            while len(minibatch_indices) < minibatch_size:\n",
    "                minibatch_end_index = (\n",
    "                minibatch_start_index + minibatch_size - len(minibatch_indices))\n",
    "                if minibatch_end_index >= num_rows:\n",
    "                    minibatch_indices += range(minibatch_start_index, num_rows)\n",
    "                    minibatch_start_index = 0\n",
    "                else:\n",
    "                    minibatch_indices += range(minibatch_start_index, minibatch_end_index)\n",
    "                    minibatch_start_index = minibatch_end_index\n",
    "\n",
    "            session.run(\n",
    "                  model.train_op,\n",
    "                  feed_dict=model.feed_dict_helper(\n",
    "                      train_df.iloc[[permutation[ii] for ii in minibatch_indices]], train=True))\n",
    "            \n",
    "        train_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        test_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(test_df))\n",
    "\n",
    "        yield (train_predictions, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing accuracy and fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def error_rate(predictions, labels):\n",
    "    signed_labels = (\n",
    "      (labels > 0).astype(np.float32) - (labels <= 0).astype(np.float32))\n",
    "    numerator = (np.multiply(signed_labels, predictions) <= 0).sum()\n",
    "    denominator = predictions.shape[0]\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def positive_prediction_rate(predictions, subset):\n",
    "    numerator = np.multiply((predictions > 0).astype(np.float32),\n",
    "                          (subset > 0).astype(np.float32)).sum()\n",
    "    denominator = (subset > 0).sum()\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def fpr(df):\n",
    "    fp = sum((df['predictions'] >= 0.0) & (df[LABEL_COLUMN] < 0.5))\n",
    "    ln = sum(df[LABEL_COLUMN] < 0.5)\n",
    "    return float(fp) / float(ln)\n",
    "\n",
    "def _get_error_rate_and_constraints(df, fpr_max_diff):\n",
    "    error_rate_local = error_rate(df[['predictions']], df[[LABEL_COLUMN]])\n",
    "    overall_fpr = fpr(df)\n",
    "    return error_rate_local, overall_fpr, [fpr(df[df[protected_attribute] > 0.5]) - (overall_fpr + fpr_max_diff) for protected_attribute in PROTECTED_COLUMNS]\n",
    "\n",
    "def _get_exp_error_rate_constraints(cand_dist, error_rates_vector,\n",
    "                                    overall_fpr_vector, constraints_matrix):\n",
    "    expected_error_rate = np.dot(cand_dist, error_rates_vector)\n",
    "    expected_overall_fpr = np.dot(cand_dist, overall_fpr_vector)\n",
    "    expected_constraints = np.matmul(cand_dist, constraints_matrix)\n",
    "    return expected_error_rate, expected_overall_fpr, expected_constraints\n",
    "\n",
    "\n",
    "def get_iterate_metrics(cand_dist, best_cand_index, error_rate_vector,\n",
    "                        overall_fpr_vector, constraints_matrix):\n",
    "    metrics = {}\n",
    "    exp_error_rate, exp_overall_fpr, exp_constraints = _get_exp_error_rate_constraints(\n",
    "      cand_dist, error_rate_vector, overall_fpr_vector, constraints_matrix)\n",
    "    metrics['m_stochastic_error_rate'] = exp_error_rate\n",
    "    metrics['m_stochastic_overall_fpr'] = exp_overall_fpr\n",
    "    metrics['m_stochastic_max_constraint_violation'] = max(exp_constraints)\n",
    "    for i, constraint in enumerate(exp_constraints):\n",
    "        metrics['m_stochastic_constraint_violation_%d' % i] = constraint\n",
    "    metrics['best_error_rate'] = error_rate_vector[best_cand_index]\n",
    "    metrics['last_error_rate'] = error_rate_vector[-1]\n",
    "    metrics['t_stochastic_error_rate'] = sum(error_rate_vector) / len(\n",
    "      error_rate_vector)\n",
    "    metrics['best_overall_fpr'] = overall_fpr_vector[best_cand_index]\n",
    "    metrics['last_overall_fpr'] = overall_fpr_vector[-1]\n",
    "    metrics['t_stochastic_overall_fpr'] = sum(overall_fpr_vector) / len(\n",
    "      overall_fpr_vector)\n",
    "    avg_constraints = []\n",
    "    best_constraints = []\n",
    "    last_constraints = []\n",
    "    for constraint_iterates in np.transpose(constraints_matrix):\n",
    "        avg_constraint = sum(constraint_iterates) / len(constraint_iterates)\n",
    "        avg_constraints.append(avg_constraint)\n",
    "        best_constraints.append(constraint_iterates[best_cand_index])\n",
    "        last_constraints.append(constraint_iterates[-1])\n",
    "    metrics['best_max_constraint_violation'] = max(best_constraints)\n",
    "    for i, constraint in enumerate(best_constraints):\n",
    "        metrics['best_constraint_violation_%d' % i] = constraint\n",
    "    metrics['last_max_constraint_violation'] = max(last_constraints)\n",
    "    for i, constraint in enumerate(last_constraints):\n",
    "        metrics['last_constraint_violation_%d' % i] = constraint\n",
    "    metrics['t_stochastic_max_constraint_violation'] = max(avg_constraints)\n",
    "    for i, constraint in enumerate(avg_constraints):\n",
    "        metrics['t_stochastic_constraint_violation_%d' % i] = constraint\n",
    "    return metrics\n",
    "\n",
    "def training_helper(model,\n",
    "                    train_df,\n",
    "                    test_df,\n",
    "                    minibatch_size,\n",
    "                    num_iterations_per_loop=1,\n",
    "                    num_loops=1):\n",
    "    train_objective_vector = []\n",
    "    train_constraints_loss_matrix = []\n",
    "    train_error_rate_vector = []\n",
    "    train_overall_fpr_vector = []\n",
    "    train_constraints_matrix = []\n",
    "    test_error_rate_vector = []\n",
    "    test_overall_fpr_vector = []\n",
    "    test_constraints_matrix = []\n",
    "    for train, test in training_generator(\n",
    "        model, train_df, test_df, minibatch_size, num_iterations_per_loop, num_loops):\n",
    "        train_df['predictions'] = train\n",
    "        test_df['predictions'] = test\n",
    "\n",
    "        if model.gen_split:\n",
    "            train_error_rate_split0, train_overall_fpr0, train_constraints_split0 = _get_error_rate_and_constraints(train_df[train_df['SPLIT_0'] > 0], model.fpr_max_diff)\n",
    "            train_error_rate_split1, train_overall_fpr1, train_constraints_split1 = _get_error_rate_and_constraints(train_df[train_df['SPLIT_1'] > 0], model.fpr_max_diff)\n",
    "            train_error_rate_vector.append(train_error_rate_split0)\n",
    "            train_constraints_matrix.append(train_constraints_split1)\n",
    "            train_constraints_loss_matrix.append(train_constraints_split1)\n",
    "            train_overall_fpr_vector.append((train_overall_fpr0 + train_overall_fpr1) / 2)\n",
    "        else:\n",
    "            train_error_rate, train_overall_fpr, train_constraints = _get_error_rate_and_constraints(train_df, model.fpr_max_diff)\n",
    "            train_error_rate_vector.append(train_error_rate)\n",
    "            train_overall_fpr_vector.append(train_overall_fpr)\n",
    "            train_constraints_matrix.append(train_constraints)\n",
    "\n",
    "        test_error_rate, test_overall_fpr, test_constraints = _get_error_rate_and_constraints(\n",
    "            test_df, model.fpr_max_diff)\n",
    "        test_error_rate_vector.append(test_error_rate)\n",
    "        test_overall_fpr_vector.append(test_overall_fpr)\n",
    "        test_constraints_matrix.append(test_constraints)\n",
    "\n",
    "    cand_dist = tfco.find_best_candidate_distribution(\n",
    "      train_error_rate_vector, train_constraints_matrix, epsilon=0.001)\n",
    "    best_cand_index = tfco.find_best_candidate_index(\n",
    "      train_error_rate_vector, train_constraints_matrix)\n",
    "    train_metrics = get_iterate_metrics(\n",
    "      cand_dist, best_cand_index, train_error_rate_vector,\n",
    "      train_overall_fpr_vector, train_constraints_matrix)\n",
    "    test_metrics = get_iterate_metrics(\n",
    "      cand_dist, best_cand_index, test_error_rate_vector,\n",
    "      test_overall_fpr_vector, test_constraints_matrix)\n",
    "\n",
    "    return (train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline without constraints.\n",
    "\n",
    "We now declare the model, build the training op, and then perform the training. We use a neural network with 10 hidden units as the underlying classifier, and train using the ADAM optimizer with learning rate 0.01, with minibatch size of 100 over 500 epochs. We first train without fairness constraints to show the baseline performance. We see that without training fair fairness, we obtain a high fairness violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\"baseline_unconstrained\", FEATURE_NAMES, hidden_units=10, gen_split=False)\n",
    "model.build_train_op(0.01, unconstrained=True)\n",
    "\n",
    "results = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=14,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.10479041916167664\n",
      "Train Violation 0.08842700774812448\n",
      "\n",
      "Test Error 0.135258358662614\n",
      "Test Violation 0.1786933268832809\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", results[0][\"last_error_rate\"])\n",
    "print(\"Train Violation\", results[0][\"last_max_constraint_violation\"])\n",
    "print()\n",
    "print(\"Test Error\", results[1][\"last_error_rate\"])\n",
    "print(\"Test Violation\", results[1][\"last_max_constraint_violation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline with constraints single training dataset.\n",
    "\n",
    "We now show train with the constraints and show the performance of the m-stochastic classifier as used in [[CotterEtAl2018]](https://arxiv.org/abs/1807.00028).\n",
    "\n",
    "We see a fairly large difference between training fairness violation and testing fairness violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\"single_dataset\", FEATURE_NAMES, hidden_units=10, gen_split=False)\n",
    "model.build_train_op(0.01, unconstrained=False)\n",
    "\n",
    "results = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=14,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.2252994011976048\n",
      "Train Violation 0.03470052884024105\n",
      "\n",
      "Test Error 0.23100303951367782\n",
      "Test Violation 0.025235830041119085\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", results[0][\"m_stochastic_error_rate\"])\n",
    "print(\"Train Violation\", results[0][\"m_stochastic_max_constraint_violation\"])\n",
    "print()\n",
    "print(\"Test Error\", results[1][\"m_stochastic_error_rate\"])\n",
    "print(\"Test Violation\", results[1][\"m_stochastic_max_constraint_violation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together: training with constraints using dataset splitting.\n",
    "\n",
    "We now show what happens when we split the training dataset into two halves, one for minimizing the loss and the other for enforcing the fairness constraints. \n",
    "\n",
    "We see a substantial improvement in generalization (that is, the difference between training fairness violation and testing fairness violation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\"split_dataset\", FEATURE_NAMES, hidden_units=10, gen_split=True)\n",
    "model.build_train_op(0.01, unconstrained=False)\n",
    "\n",
    "results = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=14,\n",
    "      num_loops=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.24011713030746706\n",
      "Train Violation 0.010979308226803331\n",
      "\n",
      "Test Error 0.22796352583586627\n",
      "Test Violation 0.004205971673519847\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", results[0][\"m_stochastic_error_rate\"])\n",
    "print(\"Train Violation\", results[0][\"m_stochastic_max_constraint_violation\"])\n",
    "print()\n",
    "print(\"Test Error\", results[1][\"m_stochastic_error_rate\"])\n",
    "print(\"Test Violation\", results[1][\"m_stochastic_max_constraint_violation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
